### 块层映像最深的问题：
现象：
```
可能出现的用例ltp dio_truncate，非必现
失败用例
[ 9307.792759] INFO: task dio_truncate:1784076 blocked for more than 120 seconds.
[ 9307.801099] Tainted: G W OE 5.10.0+ #1
[ 9307.807463] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[ 9307.816337] task:dio_truncate state:D stack: 0 pid:1784076 ppid:1784075 flags:0x00000200
[ 9307.826010] Call trace:
[ 9307.829545] __switch_to+0x7c/0xbc
[ 9307.834035] __schedule+0x424/0x850
[ 9307.838607] schedule+0x50/0xe0
[ 9307.842830] rwsem_down_write_slowpath+0x3cc/0x6cc
[ 9307.848704] down_write+0x60/0x70
[ 9307.853133] ext4_dio_write_iter+0xac/0x38c [ext4]
[ 9307.859041] ext4_file_write_iter+0x50/0x160 [ext4]
[ 9307.865021] new_sync_write+0xec/0x18c
[ 9307.869879] vfs_write+0x214/0x2ac
[ 9307.874380] ksys_write+0x70/0xfc
[ 9307.878797] __arm64_sys_write+0x24/0x30
[ 9307.883819] el0_svc_common.constprop.0+0x7c/0x1bc
[ 9307.889707] do_el0_svc+0x2c/0x94
[ 9307.894125] el0_svc+0x20/0x30
[ 9307.898287] el0_sync_handler+0xb0/0xb4
[ 9307.903225] el0_sync+0x160/0x180
[ 9307.907664] INFO: task dio_truncate:1784077 blocked for more than 120 seconds.
[ 9307.916017] Tainted: G W OE 5.10.0+ #1
[ 9307.922455] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[ 9307.931410] task:dio_truncate state:D stack: 0 pid:1784077 ppid:1784076 flags:0x00000200
[ 9307.941161] Call trace:
[ 9307.944774] __switch_to+0x7c/0xbc
[ 9307.949341] __schedule+0x424/0x850
[ 9307.953983] schedule+0x50/0xe0
[ 9307.958286] rwsem_down_read_slowpath+0x1b0/0x670
[ 9307.964144] down_read+0xa4/0xc0
[ 9307.968562] ext4_dio_read_iter+0x104/0x110 [ext4]
[ 9307.974533] ext4_file_read_iter+0xec/0x170 [ext4]
[ 9307.980493] new_sync_read+0xec/0x184
[ 9307.985325] vfs_read+0x184/0x1e0
[ 9307.989817] ksys_read+0x74/0x100
[ 9307.994308] __arm64_sys_read+0x24/0x30
[ 9307.999316] el0_svc_common.constprop.0+0x7c/0x1bc
[ 9308.005282] do_el0_svc+0x2c/0x94
[ 9308.009776] el0_svc+0x20/0x30
[ 9308.014017] el0_sync_handler+0xb0/0xb4
[ 9308.019033] el0_sync+0x160/0x180
[ 9308.023536] INFO: task dio_truncate:1784078 blocked for more than 121 seconds.
[ 9308.031940] Tainted: G W OE 5.10.0+ #1
[ 9308.038454] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[ 9308.047486] task:dio_truncate state:D stack: 0 pid:1784078 ppid:1784076 flags:0x00000200
[ 9308.057331] Call trace:
[ 9308.061038] __switch_to+0x7c/0xbc
[ 9308.065676] __schedule+0x424/0x850
[ 9308.070397] schedule+0x50/0xe0
[ 9308.074759] rwsem_down_read_slowpath+0x1b0/0x670
[ 9308.080657] down_read+0xa4/0xc0
[ 9308.085091] ext4_dio_read_iter+0x104/0x110 [ext4]
[ 9308.091086] ext4_file_read_iter+0xec/0x170 [ext4]
[ 9308.097064] new_sync_read+0xec/0x184
[ 9308.101919] vfs_read+0x184/0x1e0
[ 9308.106434] ksys_read+0x74/0x100
[ 9308.110944] __arm64_sys_read+0x24/0x30
[ 9308.115961] el0_svc_common.constprop.0+0x7c/0x1bc
[ 9308.121929] do_el0_svc+0x2c/0x94
[ 9308.126427] el0_svc+0x20/0x30
[ 9308.130662] el0_sync_handler+0xb0/0xb4
[ 9308.135678] el0_sync+0x160/0x180
```

原因：
```
handoff机制用于防止锁饿死，但在使能乐观自旋时，如下场景handoff机制不能生效：
1、读者1持锁 -- 1个读者持锁
2、读者2进来，走快速路径持锁 -- 2个读者持锁
3、写者1进来，乐观自旋失败，进入队列等待 -- 2个读者持锁，1个写者在队列中等锁
4、读者3/4/5/6...进来，乐观自旋等待 -- 2个读者持锁，1个写者在队列中等锁，多个读者乐观自旋
5、读者1放锁，因为读者2存在，读者数量非0，不会wake等待队列里的进程(wake进程才能使handoff机制生效) -- 1个读者持锁，1个写者在队列中等锁，多个读者乐观自旋
6、读者3检测到owner变化，抢到读锁 -- 2个读者持锁，1个写者在队列中等锁，多个读者乐观自旋
不断有读者进来，4/5/6陷入循环，写者长时间等不到锁
当前用例存在16个读者1个写者并发的情况，可能会触发上述问题场景
```

修复：
```
2f06f702925b512a95b95dca3855549c047eef58
locking/rwsem: Prevent potential lock starvation
在有写者等锁，且读者持有锁的时候，不允许新读者读者进行rwsem_optimistic_spin，新读者需要进入到等待队列，避免写者饿死
```

### nfsd映像最深的问题
现象
nfs访问卡住

原因

服务端回收deleg两种情况：
服务端先发送recall
1. 客户端发delegreturn返回
2. 客户端超时没有发送，服务端通过laundromat加入revoke链表，后续客户端再发送请求时，会因revoke链表非空返回异常，让客户端遍历所有deleg，返回异常的deleg

当前delegreturn与laundromat并发，有窗口会导致客户端已经通过delegreturn返回，服务端仍将deleg加入revoke链表的情况，导致后续客户端发送请求会触发所有deleg的遍历，但因为异常deleg已经返还，客户端没有这个deleg，所以每个请求都会触发所有deleg的遍历，在deleg个数多的情况下严重影响性能

```
When file access conflicts occur between clients, the server recalls
delegations. If the client holding delegation fails to return it after
a recall, nfs4_laundromat adds the delegation to cl_revoked list.
This causes subsequent SEQUENCE operations to set the
SEQ4_STATUS_RECALLABLE_STATE_REVOKED flag, forcing the client to
validate all delegations and return the revoked one.

However, if the client fails to return the delegation like this:
nfs4_laundromat                       nfsd4_delegreturn
 unhash_delegation_locked
 list_add // add dp to reaplist
          // by dl_recall_lru
 list_del_init // delete dp from
               // reaplist
                                       destroy_delegation
                                        unhash_delegation_locked
                                         // do nothing but return false
 revoke_delegation
 list_add // add dp to cl_revoked
          // by dl_recall_lru

The delegation will remain in the server's cl_revoked list while the
client marks it revoked and won't find it upon detecting
SEQ4_STATUS_RECALLABLE_STATE_REVOKED.
This leads to a loop:
the server persistently sets SEQ4_STATUS_RECALLABLE_STATE_REVOKED, and the
client repeatedly tests all delegations, severely impacting performance
when numerous delegations exist.

Since abnormal delegations are removed from flc_lease via nfs4_laundromat
--> revoke_delegation --> destroy_unhashed_deleg -->
nfs4_unlock_deleg_lease --> kernel_setlease, and do not block new open
requests indefinitely, retaining such a delegation on the server is
unnecessary.
```

复现补丁
```
diff --git a/fs/nfs/nfs4proc.c b/fs/nfs/nfs4proc.c
index 7d2b67e06cc3..2867f6afa84f 100644
--- a/fs/nfs/nfs4proc.c
+++ b/fs/nfs/nfs4proc.c
@@ -6951,6 +6951,7 @@ static int _nfs4_proc_delegreturn(struct inode *inode, const struct cred *cred,
 	task_setup_data.callback_data = data;
 	msg.rpc_argp = &data->args;
 	msg.rpc_resp = &data->res;
+	printk("%s return deleg\n", __func__);
 	task = rpc_run_task(&task_setup_data);
 	if (IS_ERR(task))
 		return PTR_ERR(task);
@@ -10663,6 +10664,7 @@ static int nfs41_free_stateid(struct nfs_server *server,
 	msg.rpc_argp = &data->args;
 	msg.rpc_resp = &data->res;
 	nfs4_init_sequence(&data->args.seq_args, &data->res.seq_res, 1, privileged);
+	printk("%s free stateid\n", __func__);
 	task = rpc_run_task(&task_setup);
 	if (IS_ERR(task))
 		return PTR_ERR(task);
diff --git a/fs/nfs/nfs4renewd.c b/fs/nfs/nfs4renewd.c
index db3811af0796..4d2a0dabdd04 100644
--- a/fs/nfs/nfs4renewd.c
+++ b/fs/nfs/nfs4renewd.c
@@ -72,7 +72,7 @@ nfs4_renew_state(struct work_struct *work)
 	last = clp->cl_last_renewal;
 	now = jiffies;
 	/* Are we close to a lease timeout? */
-	if (time_after(now, last + lease/3))
+	//if (time_after(now, last + lease/3))
 		renew_flags |= NFS4_RENEW_TIMEOUT;
 	if (nfs_delegations_present(clp))
 		renew_flags |= NFS4_RENEW_DELEGATION_CB;
@@ -118,9 +118,9 @@ nfs4_schedule_state_renewal(struct nfs_client *clp)
 	spin_lock(&clp->cl_lock);
 	timeout = (2 * clp->cl_lease_time) / 3 + (long)clp->cl_last_renewal
 		- (long)jiffies;
-	if (timeout < 5 * HZ)
-		timeout = 5 * HZ;
-	dprintk("%s: requeueing work. Lease period = %ld\n",
+//	if (timeout < 5 * HZ)
+		timeout = 20 * HZ;
+	printk("%s: requeueing work. Lease period = %ld\n",
 			__func__, (timeout + HZ - 1) / HZ);
 	mod_delayed_work(system_wq, &clp->cl_renewd, timeout);
 	set_bit(NFS_CS_RENEWD, &clp->cl_res_state);
diff --git a/fs/nfs/nfs4state.c b/fs/nfs/nfs4state.c
index 7612e977e80b..54c06c11b7f0 100644
--- a/fs/nfs/nfs4state.c
+++ b/fs/nfs/nfs4state.c
@@ -2484,8 +2484,12 @@ void nfs41_handle_sequence_flag_errors(struct nfs_client *clp, u32 flags,
 		nfs41_handle_some_state_revoked(clp);
 	if (flags & SEQ4_STATUS_LEASE_MOVED)
 		nfs4_schedule_lease_moved_recovery(clp);
-	if (flags & SEQ4_STATUS_RECALLABLE_STATE_REVOKED)
+	if (flags & SEQ4_STATUS_RECALLABLE_STATE_REVOKED) {
+		printk("%s detect SEQ4_STATUS_RECALLABLE_STATE_REVOKED\n", __func__);
 		nfs41_handle_recallable_state_revoked(clp);
+	} else {
+		printk("%s no SEQ4_STATUS_RECALLABLE_STATE_REVOKED\n", __func__);
+	}
 out_recovery:
 	if (flags & SEQ4_STATUS_BACKCHANNEL_FAULT)
 		nfs41_handle_backchannel_fault(clp);
diff --git a/fs/nfs_common/grace.c b/fs/nfs_common/grace.c
index 27cd0d13143b..6514d9e3636d 100644
--- a/fs/nfs_common/grace.c
+++ b/fs/nfs_common/grace.c
@@ -32,9 +32,10 @@ locks_start_grace(struct net *net, struct lock_manager *lm)
 	struct list_head *grace_list = net_generic(net, grace_net_id);
 
 	spin_lock(&grace_lock);
-	if (list_empty(&lm->list))
+	if (list_empty(&lm->list)) {
+		printk("%s add lm %px to grace_list\n", __func__, lm);
 		list_add(&lm->list, grace_list);
-	else
+	} else
 		WARN(1, "double list_add attempt detected in net %x %s\n",
 		     net->ns.inum, (net == &init_net) ? "(init_net)" : "");
 	spin_unlock(&grace_lock);
@@ -55,6 +56,7 @@ void
 locks_end_grace(struct lock_manager *lm)
 {
 	spin_lock(&grace_lock);
+	printk("%s remove lm %px to grace_list\n", __func__, lm);
 	list_del_init(&lm->list);
 	spin_unlock(&grace_lock);
 }
diff --git a/fs/nfsd/nfs4state.c b/fs/nfsd/nfs4state.c
index 0ce211efcebd..7ed70adaa851 100644
--- a/fs/nfsd/nfs4state.c
+++ b/fs/nfsd/nfs4state.c
@@ -4482,14 +4482,17 @@ nfsd4_sequence(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,
 		list_for_each_safe(pos, next, &clp->cl_revoked) {
 			dp = list_entry(pos, struct nfs4_delegation, dl_recall_lru);
 			if (dp->dl_stid.sc_status & SC_STATUS_CLOSED) {
+				printk("%s %d dp %px dl_time %llu now %llu nn->nfsd4_lease %llu\n", __func__, __LINE__, dp, dp->dl_time, ktime_get_boottime_seconds(), nn->nfsd4_lease);
 				list_del_init(&dp->dl_recall_lru);
 				nfs4_put_stid(&dp->dl_stid);
 			}
 		}
 		spin_unlock(&clp->cl_lock);
 	}
-	if (!list_empty(&clp->cl_revoked))
+	if (!list_empty(&clp->cl_revoked)) {
+		printk("%s set SEQ4_STATUS_RECALLABLE_STATE_REVOKED for seq\n", __func__);
 		seq->status_flags |= SEQ4_STATUS_RECALLABLE_STATE_REVOKED;
+	}
 	if (atomic_read(&clp->cl_admin_revoked))
 		seq->status_flags |= SEQ4_STATUS_ADMIN_STATE_REVOKED;
 	trace_nfsd_seq4_status(rqstp, seq);
@@ -5375,6 +5378,7 @@ static void nfsd4_cb_recall_prepare(struct nfsd4_callback *cb)
 
 	block_delegations(&dp->dl_stid.sc_file->fi_fhandle);
 
+	printk("%s recall dp %px\n", __func__, dp);
 	/*
 	 * We can't do this in nfsd_break_deleg_cb because it is
 	 * already holding inode->i_lock.
@@ -5385,6 +5389,7 @@ static void nfsd4_cb_recall_prepare(struct nfsd4_callback *cb)
 	spin_lock(&state_lock);
 	if (delegation_hashed(dp) && dp->dl_time == 0) {
 		dp->dl_time = ktime_get_boottime_seconds();
+		printk("%s add dp %px to del_recall_lru\n", __func__, dp);
 		list_add_tail(&dp->dl_recall_lru, &nn->del_recall_lru);
 	}
 	spin_unlock(&state_lock);
@@ -5451,6 +5456,7 @@ static void nfsd_break_one_deleg(struct nfs4_delegation *dp)
 	 * flc_lock) we know the server hasn't removed the lease yet, and
 	 * we know it's safe to take a reference.
 	 */
+	printk("%s queue dl_recall\n", __func__);
 	refcount_inc(&dp->dl_stid.sc_count);
 	queued = nfsd4_run_cb(&dp->dl_recall);
 	WARN_ON_ONCE(!queued);
@@ -6863,6 +6869,7 @@ nfs4_laundromat(struct nfsd_net *nn)
 	spin_lock(&state_lock);
 	list_for_each_safe(pos, next, &nn->del_recall_lru) {
 		dp = list_entry (pos, struct nfs4_delegation, dl_recall_lru);
+		printk("%s get dp %px from del_recall_lru\n", __func__, dp);
 		if (!state_expired(&lt, dp->dl_time))
 			break;
 		refcount_inc(&dp->dl_stid.sc_count);
@@ -6874,6 +6881,14 @@ nfs4_laundromat(struct nfsd_net *nn)
 		dp = list_first_entry(&reaplist, struct nfs4_delegation,
 					dl_recall_lru);
 		list_del_init(&dp->dl_recall_lru);
+		while (1) {
+			ifdebug(PROC) {
+				printk("<<<<<<<<<<<<<<<<<<<<<<<<<<< %s sleep for dp %px del <<<<<<<<<<<<<<<<<\n", __func__, dp);
+				msleep(5 * 1000);
+				continue;
+			}
+			break;
+		}
 		revoke_delegation(dp);
 	}
 
@@ -7811,6 +7826,14 @@ nfsd4_delegreturn(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,
 		goto put_stateid;
 
 	trace_nfsd_deleg_return(stateid);
+	while (1) {
+		ifdebug(SVC) {
+			printk(">>>>>>>>>> %s sleep before unhash %px >>>>>>>>>>\n", __func__, dp);
+			msleep(5 * 1000);
+			continue;
+		}
+		break;
+	}
 	destroy_delegation(dp);
 	smp_mb__after_atomic();
 	wake_up_var(d_inode(cstate->current_fh.fh_dentry));

```

复现步骤

```
服务端：
mkfs.ext4 -F /dev/sdb
mount /dev/sdb /mnt/sdb
echo "/mnt *(rw,no_root_squash,fsid=0)" > /etc/exports
echo "/mnt/sdb *(rw,no_root_squash,fsid=1)" >> /etc/exports
systemctl restart nfs-server
echo 123 > /mnt/sdb/testfile

客户端1：
mount -t nfs -o rw 192.168.6.249:/sdb /mnt/sdbb

客户端2：
mount -t nfs -o rw 192.168.6.249:/sdb /mnt/sdbb

# 服务端等待 lm 释放

客户端1：
exec 100</mnt/sdbb/testfile

服务端：
rpcdebug -m nfsd -s svc
rpcdebug -m nfsd -s proc

客户端2：
exec 100>/mnt/sdbb/testfile

服务端：
出现 "<<<<<<<<<<<<<<<<<<<<<<<<<<< nfs4_laundromat sleep for dp ffff9ac1dc5adac8 del <<<<<<<<<<<<<<<<<" 打印时执行
rpcdebug -m nfsd -c svc

下一次再出现 "<<<<<<<<<<<<<<<<<<<<<<<<<<< nfs4_laundromat sleep for dp ffff9ac1dc5adac8 del <<<<<<<<<<<<<<<<<" 时执行
rpcdebug -m nfsd -c proc
```

### 磁电盘映像最深的问题

> erofs合并镜像时先从高层镜像开始处理，依次将没有加入mergedir的结点加入mergedir
> 如果发现有重复的目录，则合并目录。如果重复目录有opaque标记，则跳过这个目录。但如果第一个镜像中的目录没有opaque，中间镜像的目录有，中间镜像的目录的opaque就不会被处理，导致这个中间镜像下次镜像中的同名目录，预期会被跳过，但实际会被合并

#### 复现流程

归档阈值设置为100M
写入文件每个为60M

```
[root@fedora random_files]# ls src/
random_file01  random_file03  random_file05  random_file07  random_file09
random_file02  random_file04  random_file06  random_file08
[root@fedora random_files]#
```

1、将包含9个文件的目录写入

```
cp -a src/ /mnt/fuse_dir/

[root@fedora random_files]# ls /mnt/fuse_dir/
[root@fedora random_files]# cp -a src/ /mnt/fuse_dir/
cp: preserving times for '/mnt/fuse_dir/src/random_file01': Function not implemented
cp: preserving times for '/mnt/fuse_dir/src/random_file02': Function not implemented
cp: preserving times for '/mnt/fuse_dir/src/random_file03': Function not implemented
cp: preserving times for '/mnt/fuse_dir/src/random_file04': Function not implemented
cp: preserving times for '/mnt/fuse_dir/src/random_file05': Function not implemented
cp: preserving times for '/mnt/fuse_dir/src/random_file06': Function not implemented
cp: preserving times for '/mnt/fuse_dir/src/random_file07': Function not implemented
cp: preserving times for '/mnt/fuse_dir/src/random_file08': Function not implemented
cp: preserving times for '/mnt/fuse_dir/src/random_file09': Function not implemented
cp: preserving times for '/mnt/fuse_dir/src': Function not implemented
[root@fedora random_files]# ls /mnt/fuse_dir/src/
random_file01  random_file03  random_file05  random_file07  random_file09
random_file02  random_file04  random_file06  random_file08
[root@fedora random_files]# ls /mnt/sdd/overlay_dir/
57adc027-a6de-4587-b759-c24d6e5a2f6c/ tmp/
[root@fedora random_files]# ls /mnt/sdd/overlay_dir/57adc027-a6de-4587-b759-c24d6e5a2f6c/ovl/
arc_00_04  dir_05  merge  work
[root@fedora random_files]#
```

2、整个目录删除
`rm -rf /mnt/fuse_dir/src/`

3、新建目录
`mkdir /mnt/fuse_dir/src`

4、向目录中依次拷贝文件
```
[root@fedora random_files]# cp random_file01 /mnt/fuse_dir/src/
[root@fedora random_files]# cp random_file09 /mnt/fuse_dir/src/
[root@fedora random_files]# ls /mnt/fuse_dir/src/
random_file01  random_file09
[root@fedora random_files]# cp random_file02 /mnt/fuse_dir/src/
[root@fedora random_files]# cp random_file03 /mnt/fuse_dir/src/
[root@fedora random_files]# cp random_file04 /mnt/fuse_dir/src/
[root@fedora random_files]# cp random_file05 /mnt/fuse_dir/src/
[root@fedora random_files]# ls /mnt/fuse_dir/src/
random_file01  random_file02  random_file03  random_file04  random_file05  random_file09
[root@fedora random_files]# cp random_file06 /mnt/fuse_dir/src/
[root@fedora random_files]# ls /mnt/fuse_dir/src/
random_file01  random_file03  random_file05  random_file07  random_file09
random_file02  random_file04  random_file06  random_file08
[root@fedora random_files]#
拷贝完 random_file06 后多出了 random_file07 和 random_file08
```

#### 代码流程
```
inode->opaque

liberofs_merge
 cfg.c_img_path = strdup(opt->base_meta);
 cfg.c_src_path = strdup(opt->inc_meta_vec[0]);
 rebuild_mode = true
 import_rebuild_src
  erofs_dev_open
  list_add // 将 erofs_sb_info 加入 rebuild_src_list 链表
 erofs_mkfs
  erofs_rebuild_make_root // 分配一个 root inode ，作为 mergedir ，目标根节点
  erofs_mkfs_rebuild_load_trees // rebuild_mode
   // 遍历 rebuild_src_list 链表上的所有镜像
   erofs_rebuild_load_tree
    erofs_read_superblock // 读取超级块 erofs_super_block
    erofs_read_inode_from_disk // 根据 nid 计算当前镜像的 root inode 在镜像中的位置并读取， nid 来源于 super block 中的 root_nid
	erofs_iterate_dir // 基于 root inode 遍历目录树，循环 遍历完 dir->i_size 大小范围
	 erofs_pread // 读一个 block
	 traverse_dirents // 循环 遍历完当前 block 范围内的子目录
	  erofs_rebuild_dirent_iter // ctx->cb 												处理需要合并的镜像中的文件
	   erofs_rebuild_get_dentry
	    erofs_d_lookup // 从父节点的 i_subdirs 链表中查找
	    erofs_rebuild_mkdir // 对于目录，在父节点的 i_subdirs 链表中找不到的情况
	     erofs_new_inode // 分配新的 erofs_inode 并初始化
		 erofs_d_alloc // 分配一个新的 erofs_dentry 插入父节点的 i_subdirs 链表
		 d->type = EROFS_FT_DIR // 设置 erofs_dentry 类型
		 d->inode = inode // 关联 erofs_dentry 和 erofs_inode
	   erofs_read_xattrs_from_disk
	    erofs_listxattr // 获取文件扩展属性
		inode->opaque = true // 如果有 OVL_XATTR_OPAQUE 则设置 inode->opaque
	   d->inode = inode // 关联 erofs_dentry 和 erofs_inode
	   if (!S_ISDIR(d->inode->i_mode) || d->inode->opaque) // 当前 inode 是有 opaque 属性的目录，则跳过下层子目录的遍历
														   // 应该跳过下层镜像的遍历？
  erofs_rebuild_dump_tree
   erofs_mkfs_build_tree
    __erofs_mkfs_build_tree
	 erofs_mkfs_dump_tree
	  erofs_rebuild_handle_inode
	   erofs_rebuild_load_basedir
	    erofs_read_inode_from_disk
		erofs_iterate_dir
		 traverse_dirents
		  erofs_rebuild_basedir_dirent_iter // ctx->cb									处理基础镜像中的文件
		   erofs_rebuild_get_dentry
		    erofs_d_alloc // 直接插入
	  erofs_mkfs_go
  erofs_rebuild_cleanup
   list_del // 将 erofs_sb_info 从 rebuild_src_list 链表中移除
```

#### 过程分析
```
整个复现过程包含两次镜像合并：
1、meta.00~meta.04合并成meta.00_04
2、meta.00_04和meta.05~meta.08合并成meta.00_08
问题发生在第二次镜像合并时
	Line 351: [2025-09-18 17:08:00]  <D> erofs: erofs_mkfs_rebuild_load_trees() Line[1218] load tree from /mnt/sdd/overlay_dir/57adc027-a6de-4587-b759-c24d6e5a2f6c/cache/0/meta.04
	Line 419: [2025-09-18 17:08:00]  <D> erofs: erofs_mkfs_rebuild_load_trees() Line[1218] load tree from /mnt/sdd/overlay_dir/57adc027-a6de-4587-b759-c24d6e5a2f6c/cache/0/meta.03
	Line 454: [2025-09-18 17:08:00]  <D> erofs: erofs_mkfs_rebuild_load_trees() Line[1218] load tree from /mnt/sdd/overlay_dir/57adc027-a6de-4587-b759-c24d6e5a2f6c/cache/0/meta.02
	Line 509: [2025-09-18 17:08:01]  <D> erofs: erofs_mkfs_rebuild_load_trees() Line[1218] load tree from /mnt/sdd/overlay_dir/57adc027-a6de-4587-b759-c24d6e5a2f6c/cache/0/meta.01
	Line 1041: [2025-09-18 17:10:53]  <D> erofs: erofs_mkfs_rebuild_load_trees() Line[1218] load tree from /mnt/sdd/overlay_dir/57adc027-a6de-4587-b759-c24d6e5a2f6c/cache/0/meta.08
	Line 1104: [2025-09-18 17:10:53]  <D> erofs: erofs_mkfs_rebuild_load_trees() Line[1218] load tree from /mnt/sdd/overlay_dir/57adc027-a6de-4587-b759-c24d6e5a2f6c/cache/0/meta.07
	Line 1159: [2025-09-18 17:10:53]  <D> erofs: erofs_mkfs_rebuild_load_trees() Line[1218] load tree from /mnt/sdd/overlay_dir/57adc027-a6de-4587-b759-c24d6e5a2f6c/cache/0/meta.06
	Line 1194: [2025-09-18 17:10:53]  <D> erofs: erofs_mkfs_rebuild_load_trees() Line[1218] load tree from /mnt/sdd/overlay_dir/57adc027-a6de-4587-b759-c24d6e5a2f6c/cache/0/meta.05

erofs合并镜像时先从高层的待合并镜像开始处理，依次将没有加入mergedir的结点加入mergedir的i_subdirs链表中，最后处理base镜像。
如图中依次处理meta.08~meta.05
	Line 1046: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[314] parsing //src
	Line 1049: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[364] new inode from sb /mnt/sdd/overlay_dir/57adc027-a6de-4587-b759-c24d6e5a2f6c/cache/0/meta.08
	Line 1058: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[314] parsing //src/random_file04
	Line 1061: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[364] new inode from sb /mnt/sdd/overlay_dir/57adc027-a6de-4587-b759-c24d6e5a2f6c/cache/0/meta.08
	Line 1078: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[314] parsing //src/random_file05
	Line 1081: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[364] new inode from sb /mnt/sdd/overlay_dir/57adc027-a6de-4587-b759-c24d6e5a2f6c/cache/0/meta.08
	Line 1099: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[314] parsing //src/random_file06
	Line 1102: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[364] new inode from sb /mnt/sdd/overlay_dir/57adc027-a6de-4587-b759-c24d6e5a2f6c/cache/0/meta.08
	Line 1109: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[314] parsing //src
	Line 1111: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[342] got src opaque 0
	Line 1112: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[348] got src opaque 0
	Line 1117: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[314] parsing //src/random_file02
	Line 1120: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[364] new inode from sb /mnt/sdd/overlay_dir/57adc027-a6de-4587-b759-c24d6e5a2f6c/cache/0/meta.07
	Line 1137: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[314] parsing //src/random_file03
	Line 1140: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[364] new inode from sb /mnt/sdd/overlay_dir/57adc027-a6de-4587-b759-c24d6e5a2f6c/cache/0/meta.07
	Line 1157: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[314] parsing //src/random_file04
	Line 1164: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[314] parsing //src
	Line 1166: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[342] got src opaque 0
	Line 1167: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[348] got src opaque 0
	Line 1172: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[314] parsing //src/random_file02
	Line 1175: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[314] parsing //src/random_file09
	Line 1178: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[364] new inode from sb /mnt/sdd/overlay_dir/57adc027-a6de-4587-b759-c24d6e5a2f6c/cache/0/meta.06
	Line 1199: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[314] parsing //src
	Line 1201: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[342] got src opaque 0
	Line 1203: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[348] got src opaque 1
	Line 1208: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[314] parsing //src/random_file01
	Line 1211: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[364] new inode from sb /mnt/sdd/overlay_dir/57adc027-a6de-4587-b759-c24d6e5a2f6c/cache/0/meta.05
	Line 1228: [2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_dirent_iter() Line[314] parsing //src/random_file09

在遍历镜像文件(目录)的过程中，如果i_subdirs中没有该文件(目录)，则新生成结点插入(src目录在遍历meta.08时插入，其他文件在遍历后续镜像时插入)
	Line 1036: [2025-09-18 17:10:53]  <D> erofs: erofs_d_alloc() Line[189] add . to root
	Line 1038: [2025-09-18 17:10:53]  <D> erofs: erofs_d_alloc() Line[189] add .. to root
	Line 1047: [2025-09-18 17:10:53]  <D> erofs: erofs_d_alloc() Line[189] add src to root
	Line 1050: [2025-09-18 17:10:53]  <D> erofs: erofs_d_alloc() Line[189] add . to root
	Line 1052: [2025-09-18 17:10:53]  <D> erofs: erofs_d_alloc() Line[189] add .. to root
	Line 1059: [2025-09-18 17:10:53]  <D> erofs: erofs_d_alloc() Line[189] add random_file04 to root
	Line 1079: [2025-09-18 17:10:53]  <D> erofs: erofs_d_alloc() Line[189] add random_file05 to root
	Line 1100: [2025-09-18 17:10:53]  <D> erofs: erofs_d_alloc() Line[189] add random_file06 to root
	Line 1118: [2025-09-18 17:10:53]  <D> erofs: erofs_d_alloc() Line[189] add random_file02 to root
	Line 1138: [2025-09-18 17:10:53]  <D> erofs: erofs_d_alloc() Line[189] add random_file03 to root
	Line 1176: [2025-09-18 17:10:53]  <D> erofs: erofs_d_alloc() Line[189] add random_file09 to root
	Line 1209: [2025-09-18 17:10:53]  <D> erofs: erofs_d_alloc() Line[189] add random_file01 to root
	Line 1265: [2025-09-18 17:10:53]  <D> erofs: erofs_d_alloc() Line[189] add random_file07 to root
	Line 1269: [2025-09-18 17:10:53]  <D> erofs: erofs_d_alloc() Line[189] add random_file08 to root
如果发现有重复的目录，且已有的目录结点没有opaque，则合并目录(erofs_rebuild_dirent_iter中的逻辑实现)

在处理meta.08时生成src结点插入链表，由于meta.08中src目录没有opaque属性，则插入结点的链表也没有该属性，后续所有同名目录都会被合并

在处理meta.05时，发现已有src结点，会直接进行合并操作，虽然meta.05中src目录有opaque属性，用于屏蔽下层镜像中的同名目录，但这个属性并不会被读取，这就导致最后在合并base镜像时会将镜像中的src目录进行合并

如图在处理最后base镜像时，由于检查到src目录并没有opaque属性，因此会遍历base镜像中src目录的所有文件，其中randomfile_06和random_file07在上层镜像中都没有，这里就会生成新的结点插入。
但我们的预期是meta.05中的src结点所携带的opaque属性会在遍历meta.05后续镜像时被设置在mergedir中的src结点上，从而在处理base镜像时直接跳过base镜像中的src目录
[2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_get_dentry() Line[126] got random_file05 opaque 0
[2025-09-18 17:10:53]  <D> erofs: traverse_dirents() Line[49] traversed nid (176) random_file06
[2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_basedir_dirent_iter() Line[500] ctx->dname random_file06random_file07random_file08random_file09
[2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_get_dentry() Line[126] got random_file06 opaque 0
[2025-09-18 17:10:53]  <D> erofs: traverse_dirents() Line[49] traversed nid (185) random_file07
[2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_basedir_dirent_iter() Line[500] ctx->dname random_file07random_file08random_file09
[2025-09-18 17:10:53]  <D> erofs: erofs_d_alloc() Line[189] add random_file07 to root
[2025-09-18 17:10:53]  
[2025-09-18 17:10:53]  <D> erofs: traverse_dirents() Line[49] traversed nid (194) random_file08
[2025-09-18 17:10:53]  <D> erofs: erofs_rebuild_basedir_dirent_iter() Line[500] ctx->dname random_file08random_file09
[2025-09-18 17:10:53]  <D> erofs: erofs_d_alloc() Line[189] add random_file08 to root
[2025-09-18 17:10:53]  
[2025-09-18 17:10:53]  <D> erofs: traverse_dirents() Line[49] traversed nid (201) random_file09
```
