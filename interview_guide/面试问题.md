# io_uring

## 一、入门级：基本概念与背景

### Q1：什么是 io_uring？为什么引入它？
**A：**
io_uring 是 Linux 5.1 引入的异步 I/O 框架，由 Jens Axboe 设计，用于减少系统调用次数和用户态/内核态切换开销。相比传统 AIO（libaio），它性能更高、更灵活，能够支持广泛的异步操作类型。

---

### Q2：io_uring 与传统 read/write 系统调用的主要区别是什么？
**A：**
- 传统 read/write 是同步阻塞的系统调用；
- io_uring 采用共享环形缓冲区的异步提交/完成机制：
  - 用户进程提交请求至 Submission Queue（SQ）；
  - 内核完成后将结果写入 Completion Queue（CQ）；
  - 用户可异步获取结果，无需阻塞等待。

---

## 二、进阶级：架构与机制

### Q3：io_uring 的主要数据结构是什么？
**A：**
1. **SQ（Submission Queue）**：提交请求的环形队列。
2. **CQ（Completion Queue）**：返回完成事件的队列。
3. **SQE（Submission Queue Entry）**：一次 I/O 请求描述。
4. **CQE（Completion Queue Entry）**：I/O 完成的结果项。
5. **SQ索引数组**：保存SQE的索引，当前与SQE是一一对应关系。
---

### Q4：io_uring 如何减少系统调用开销？
**A：**
- 使用 `mmap` 映射共享 SQ/CQ 到用户空间；
- 通过单次 `io_uring_enter()` 批量提交/等待多个请求；
- 支持链式操作（linked SQEs）以减少上下文切换。

---

### Q5：io_uring 支持哪些类型的操作？
**A：**
支持文件、网络、超时、事件、设备命令等广泛操作：
- `readv/writev`, `recv/send`, `accept/connect`
- `fsync`, `poll`, `timeout`, `splice`, `openat`, `close`
- `uring_cmd` 扩展支持驱动层命令（如块设备）。

---

## 三、高级：性能优化与内核机制

### Q6：io_uring 的零拷贝机制（fixed buffer）是怎么实现的？
**A：**
- 用户通过 `IORING_REGISTER_BUFFERS` 注册内存缓冲区；
- I/O 直接引用这些缓冲区（`IOSQE_FIXED_BUF`），避免频繁 pin/unpin 和内存拷贝；
- 实现近似零拷贝的数据传输。

---

### Q7：io_uring 如何支持多线程并发？
**A：**
- SQ/CQ 基于无锁环形队列设计；
- 各线程可安全地并发提交；
- 内核通过 io-wq 线程池处理阻塞型操作；
- 非阻塞操作可在内核上下文中直接完成。

---

### Q8：io-wq 线程池的作用是什么？
**A：**
- 用于执行无法立即完成的阻塞操作；
- 动态调整线程数量；
- 可结合 `IORING_SETUP_SQPOLL` 或 `IORING_SETUP_IOPOLL` 控制行为；
- 避免应用线程因阻塞而降低并发性能。

---

## 四、深入：I/O 路径与内核交互

### Q9：DIO（Direct I/O）与 io_uring 的关系？
**A：**
- DIO 绕过页缓存，直接在用户缓冲区与设备间 DMA；
- io_uring 结合 DIO 可实现真正的用户态→设备异步路径；
- 适合数据库、KV 存储等低延迟场景。

---

### Q10：io_uring 与 AIO（libaio） 的关键改进对比

| 特性 | AIO | io_uring |
|------|------|-----------|
| 模型 | 部分异步（仅 O_DIRECT） | 真异步 |
| 系统调用 | 每次提交都 syscall | 批量提交，共享环 |
| 支持类型 | 仅文件 | 文件、socket、event 等 |
| 线程模型 | kthread | io-wq 线程池 |
| 性能 | 较差 | 高，接近 epoll + non-block |

---

## 五、专家级：优化与问题定位

### Q11：哪些场景 io_uring 反而性能不佳？
**A：**
- 小量 I/O 或低并发；
- 存储延迟高的 HDD；
- 使用页缓存命中率高的场景；
- 固定缓冲区使用不当导致队列拥塞。

---

### Q12：如何排查 io_uring 程序中 I/O 挂起？
**A：**
- 检查 CQ head/tail 是否更新；
- 确认 `IORING_ENTER_GETEVENTS` 调用逻辑；
- 检查 `IOSQE_IO_LINK` 链式请求是否中断；
- 确认 SQPOLL 线程是否退出；
- 调用 `/proc/<pid>/fdinfo` 及 `trace_event io_uring:*` 调试。

---

### Q13：如何用 io_uring 优化 Redis/MySQL？
**A：**
- 后台写日志使用 io_uring + fixed buffer；
- 顺序大块 I/O 使用 DIO；
- 开启 SQPOLL 减少 wakeup；
- 实现自定义多路复用（timeout/poll）；
- 每线程独立 ring，避免锁竞争。

---

## 六、附加加分题

### Q14：描述 io_uring read 的路径（以 ext4 + DIO 为例）
**A：**
1. 用户提交 SQE（op=READ）；
2. 内核从 SQ 获取请求；
3. 通过 `vfs_read_iter()` → `ext4_file_read_iter()`；
4. ext4 调用 `iomap_dio_rw()` → `submit_bio()`；
5. 块层发往驱动；
6. I/O 完成后 bio_end_io → CQE 返回用户态。

---

### Q15：io_uring 的未来发展方向
**A：**
- 统一异步 API（文件、网络、驱动）；
- 深度内核集成（块层原生 io_uring 支持）；
- eBPF 与 io_uring_cmd 融合；
- 用户态 I/O 框架内核化趋势（如 SPDK/DPDK）。

---

## 总结

| 层次 | 目标 | 评估内容 |
|------|------|-----------|
| 入门 | 理解 io_uring 概念 | 异步模型基础 |
| 进阶 | 理解结构与机制 | SQ/CQ 流程 |
| 高级 | 掌握并发与优化 | io-wq、poll 模式 |
| 深入 | 理解 I/O 路径 | 文件系统交互 |
| 专家 | 性能与定位 | 实际系统优化方案 |

# nfs客户端在打开文件的情况下，服务端修改文件后，客户端怎么知道缓存数据是旧的
如果有其他客户端打开文件，服务端会做delegation recall；
当前客户端通过getattr，重新open，或者缓存超时来判断当前缓存数据是旧的（Linux NFS 客户端对 inode 维护一组时间点/有效期）

# 内存拷贝的时候为什么要内存对齐，用户态通过系统调用传递给内核的数据为什么要有一次拷贝动作
> 为什么要对齐
1. 不对齐的地址拷贝可能使得本来一次就能全部拷贝的数据要分两次拷贝
2. 未对齐会导致多映射/多段，硬件散列表（scatter-gather）条目变多（NVME获取主机侧缓存数据？）
- 让 CPU/设备/IOMMU 用最省事的方式搬数据，很多时候是 DMA 的硬门槛

> 用户态数据为什么要拷贝
1. 安全考虑，确保数据内容在整个系统调用期间稳定
2. 生命周期考虑，异步IO可以在系统调用返回后，用户态释放内存的情况下继续执行IO操作（针对小数据或不方便 pin page的场景）
- 是内核在安全、生命周期、硬件限制之间做的折衷；零拷贝不是默认，而是满足一堆条件后才能拿到的奖励


# 介绍一下hungtask的原理
khungtaskd
1. 周期性扫描系统里的任务列表（所有进程/线程）
2. 对每个 task：
- 看它是不是处于“我关心的阻塞态”（D / killable 等）
- 看它有没有调度运行过
- 看它在这个状态里已经持续多久
如果长时间处于D状态且没有调度运行过，就打印警告

## 如果一个进程在R和D状态之间切换，每次khungtaskd扫描的时候刚好在D状态，也会触发hungtask吗
不会因为“每次扫描刚好看到 D”就触发；触发通常意味着：在超时时间内它没有被调度运行过（没进展），而不只是“那一刻在 D”。
task_struct 里面有调度次数的记录，可以根据这个记录确认进程是否调度过

`sysctl_hung_task_detect_count` 检查hungtask进程数


# 介绍一下soft lockup的原理
看门狗线程watchdog是由内核创建的percpu线程，创建后一直睡眠，然后等待htimer周期性的唤醒自己。被唤醒后watchdog线程就会去“喂狗”，即将当前时间戳写入到percpu变量watchdog_touch_ts中。
percpu变量watchdog_touch_ts的更新和softlockup的检测是两种方式：
1. watchdog_touch_ts更新
通过异步进程更新，如果当前CPU没有softlockup，就可以异步执行更新函数来更新 watchdog_touch_ts ，否则 watchdog_touch_ts 无法更新
2. softlockup检测
由于定时器是通过中断触发每个CPU执行percpu线程，因此即使发生了softlockup，也可以进行检查。如果发生了 softlockup ，前面无法异步更新 watchdog_touch_ts ，那么此时就检测到 softlockup ，否则检查通过


# 介绍一下hard lockup的原理
不能正常响应NMI中断
1. NMI能进来，但CPU不能处理
通过perf event检测
2. NMI不能进来
通过其他正常的CPU检测


# 介绍一下rcu的原理（rcu stall）
核心目标：让“读”几乎不加锁、非常快；把复杂度转移到“写/更新”那边。它适合那种“读多写少、读路径必须极快”的共享数据结构

1. 读者不阻塞：读者进入“RCU 读临界区”后，可以无锁地读指针、遍历链表。
2. 写者不就地改：更新不是在原对象上改，而是复制一份新对象，改新对象，再把全局指针一次性切过去（原子指针替换）。
3. 延迟回收旧对象：旧对象不能立刻 free，必须等一个**宽限期（grace period）**过去：确认“所有可能仍在读旧对象的读者都结束了”，再释放旧对象。

宽限期的定义：
> 从某个时刻开始，等到系统里 所有 CPU 都经历过至少一次“RCU 读临界区不可能继续持有旧引用”的状态，就算宽限期结束。

## rcu_read_lock禁止抢占后，会不会有中断导致 CPU 上下文切换？
会有中断，但中断本身通常不会导致“进程上下文切换”，真正的“进程上下文切换”要靠调度器 schedule() 发生。
rcu_read_lock() 禁的是抢占，不禁中断；中断会来，但一般不会导致 task 切换

## rcu_read_unlock() 后读者退出临界区；如果当前 CPU 之后卡住不切换，会不会阻塞写者释放资源？
写者等的是“所有读者都退出临界区/被确认不会再引用旧对象”，不是等某个 CPU 一定要发生一次 context switch，根据 __rcu_read_lock() 的不同实现分两种情况。
1. 只关抢占
有可能一直阻塞，因为此时写者判断读者是否退出临界区的依据是CPU有没有切换上下文

2. rcu_read_lock_nesting
执行 rcu_read_unlock() 后 rcu_read_lock_nesting 归零，对于写者而言读者已经退出了临界区，不阻塞写者释放资源

## 写者怎么判断读者是否离开了临界区
写者不直接“盯切换”；非抢占式靠调度器/idle/tick 报告 quiescent state，PREEMPT_RCU 则靠 per-task 的读锁计数/blocked readers 判断。

srcu 原理
https://blog.csdn.net/qq_39665253/article/details/153770913
SRCU不依赖CPU的调度状态，而是为每一个被保护的数据结构实例维护一个struct srcu_struct。这个结构体内部有一个小数组（通常是两个元素）的计数器和一个当前“活动”计数器的索引，根据使用待释放资源的进程数来判断是否可以释放

rcu stall 原理？

# 介绍一下活锁和死锁
1. 死锁
   多个线程/进程之间形成了循环等待：每个人都拿着一部分资源，同时等着别人手里的资源，于是永远等下去
2. 活锁
   线程没有阻塞，反而不断运行、不断重试、不断“礼让”，但因为策略/时序问题，总在互相干扰，导致系统整体没有进展。常见于：trylock + 失败就立刻释放/重试；冲突检测后双方都回滚并同时重试

# 关于IO慢的现象，可能是什么原因，要怎么定位，展开说说

```
定位工具 iostat blktrace strace 火焰图 bpftrace vmstat 
业务自身问题： 
   业务逻辑：并发度太低，小IO太多，小文件海量
系统状态问题：
   内存：内存不足，脏页回写占IO带宽，缓存未命中 
   文件系统：元数据争用，日志提交，overlayfs copy-up 
   块层：设备参数（nr_request，调度器），cgroup限速 
   设备层：RAID重建，dm-thin-pool空间不足，碎片化严重
   硬件层：scsi设备进错误处理流程，进行设备恢复操作
```

# 介绍一下nfs的挂载流程
先 EXCHANGE_ID 建立 client identity 并获得 clientid/能力协商；再 CREATE_SESSION（以及可能的 BIND_CONN_TO_SESSION）建立 session/slot 的并发控制与 callback/backchannel 能力；然后从 PUTROOTFH 进入 v4 的 pseudo-fs 命名空间，沿 export path LOOKUP 到导出点并 GETFH，拿到导出根句柄作为后续 pathname 解析与文件操作的起点

# 介绍一下blk-wbt原理
持续观测读请求完成延迟，并用反馈控制动态调整允许的写入并发度

# 介绍一下spdk/dpdk
SPDK 走用户态 NVMe 驱动路径：通过内核把 PCIe 设备 BAR 映射到用户态，并完成 DMA 内存映射；数据面由用户态直接管理 NVMe 队列、提交/回收命令，数据用 DMA 直接进出用户态内存，从而绕开内核块层与系统调用/中断等开销，获得更低延迟与更高吞吐（代价是常常需要专用 CPU 核做轮询）

DPDK 通过用户态直接操作网卡 PCIe 设备来完成收发包，依赖内核提供的控制面接口（例如 VFIO/UIO、hugepage、IOMMU 等）来完成设备映射与安全隔离。数据收发主要基于 DMA 将报文直接搬运到用户态预分配的内存缓冲区（mbuf），再配合轮询（polling）方式从网卡队列收包/发包，从而减少中断、减少内核协议栈处理开销，并显著降低用户态/内核态切换与系统调用带来的性能损耗，实现更高吞吐和更低时延。

<img width="1005" height="681" alt="image" src="https://github.com/user-attachments/assets/e3c5f93c-8474-4bac-b99c-b4423e2b2e78" />
SPDK层次：
第一层：应用协议层，如iscsi，对外提供iscsi设备？
第二层：存储服务层，如raid-0，整合底层设备？
第三层：抽象了通用的块存储设备bdev
第四层：驱动层，SPDK实现了用户态驱动用来加速各类存储应用

## SPDK rpc client/server
SPDK 很多时候是“一个跑起来的服务进程”,典型代表就是 spdk_tgt（SPDK target framework）。它启动后会一直跑着，里面可以挂：NVMe-oF target（对外提供 NVMe over Fabrics）, iSCSI target等
RPC client 很多时候是scripts/rpc.py 这种管理工具。
SPDK 的 RPC client/server 是为了控制面：用来动态配置、管理、监控 SPDK target 进程；真正的数据读写 I/O 不走 RPC

## SPDK的reactor、events、poller和io channel机制
SPDK 的运行时可以理解成 **per-core reactor + poller 驱动的数据面**：每个 CPU 核上跑一个 **reactor 事件循环**，循环里会处理一次性的 **events（初始化/调度/配置动作）**，以及持续运行的 **pollers（轮询 NVMe completion、推进状态机）**。为了避免锁竞争，SPDK 用 **I/O channel** 把模块的 I/O 上下文做成 **per-thread 私有**（例如每个 thread 绑定自己的 NVMe qpair），这样一次 I/O 的提交和完成收割通常都在同一个 thread 上闭环，既减少了内核态/用户态切换，也减少了共享数据结构加锁带来的开销和抖动。

## SPDK线程模型
SPDK 的线程模型可以理解成一句话：**“OS 线程跑在 CPU 核上轮询，SPDK thread 是用户态的逻辑线程，靠消息和 poller 协作式执行”**。
更具体一点：
* **reactor（每核一个）**：SPDK 通常把一个 OS 线程绑到一个 CPU 核上，这个线程运行 reactor 循环，不停执行 poller（poller 逻辑上归 SPDK thread 管，物理上由 reactor 的 OS 线程执行）、处理消息队列。它不是“睡眠等唤醒”，而是偏 **busy polling** 的模型。
* **SPDK thread（逻辑线程，不等于 pthread）**：SPDK 自己实现了“线程”的概念，它不是内核调度的线程，而是一个**执行上下文/调度域**。一个 reactor 上可以挂多个 SPDK thread，reactor 在循环里把 CPU 时间片分给它们执行（协作式，不抢占）。
* **消息传递代替共享锁**：SPDK 强调“数据归属某个 thread”。跨 thread 操作通常不直接加锁改共享结构，而是用 `spdk_thread_send_msg()` 把函数投递到目标 thread 去执行，保证回调在确定的上下文里跑，减少锁竞争。
* **I/O channel（per-thread 资源）**：为了进一步减少共享，很多模块会给每个 SPDK thread 分配自己的 io channel（比如绑定自己的 NVMe qpair），让 I/O 提交与 completion 回调尽量都在同一个 SPDK thread 内闭环完成。
所以 SPDK 的整体效果是：**用“绑核轮询 + 用户态逻辑线程 + 消息驱动 + per-thread channel”换来极低的延迟抖动和极高吞吐**，代价是需要专用 CPU 核、对 NUMA/绑核/hugepage 等更敏感。

# 介绍一下透明大页
透明大页（THP, Transparent Huge Pages）可以理解成内核在“尽量不让你操心”的前提下，自动把普通 4KB 页合并成更大的页（比如 2MB）来映射内存，从而减少 TLB miss、降低页表开销，让内存访问更高效，尤其对大块连续内存读写、数据库、缓存这类负载更友好。它“透明”的意思是应用不用显式申请 hugepage，内核会在合适时机做分配/合并（以及必要时拆分），但代价也可能出现：为了凑出连续大页会触发内存整理（compaction）带来抖动，或者在频繁写入/分裂场景下引入额外开销，所以很多性能敏感业务会根据负载特征选择开启/关闭或只对特定内存区域启用。

# 介绍一下不同nfs版本的差别
* **4.0：统一状态与语义**（open/lock/lease、compound、delegation、ACL、安全）
* **4.1：会话与并行**（sessions/slot、trunking、pNFS）
* **4.2：存储 offload 与高级语义**（COPY/CLONE、READ_PLUS、ALLOCATE/DEALLOCATE、SEEK、labels、pNFS 增强）

# 介绍一下raft和paxos以及两者的区别
Raft 和 Paxos 都是在“会丢包、会超时、节点会宕机”的世界里，让一群机器**对同一串决策达成一致**的协议。它们解决的是同一个核心问题：**分布式共识（consensus）**——比如“这条写入到底算不算提交？”“谁是当前领导者？”“日志顺序到底是哪一个？”
## Raft：把共识拆成三件事，让人类也能看懂
Raft 的设计目标之一就是“可理解”。它把共识拆成三块：
1. **领导者选举（Leader Election）**
* 集群里任何时刻最多一个 Leader（理想情况）。
* 用 **term（任期）** 这个单调递增的编号做时代划分。
* 超时后，Follower 变 Candidate 拉票（RequestVote）；拿到多数票就当选。

2. **日志复制（Log Replication）**
* 只有 Leader 接受客户端写请求，把命令追加到自己的 log。
* 然后通过 AppendEntries RPC 把日志复制给 Follower。
* 当某条日志被**多数节点写入**，Leader 才认为它“已提交（committed）”，再应用到状态机，并通知 follower 提交。

3. **安全性（Safety）**
   Raft 有一条很关键的直觉：**新 Leader 必须拥有所有已提交的日志**。
* 选举时通过“投票限制”确保：日志更落后的候选人拿不到票（避免提交过的记录丢失）。
* 日志冲突用“回退匹配”修正：Follower 与 Leader 的日志如果在某个 index/term 不一致，就删掉后面的冲突部分，跟随 Leader。

## Paxos：数学家写的共识诗，简洁但让工程师掉头发
经典 Paxos（Single-Decree Paxos）解决的是：**就一个值达成一致**（比如“本轮选哪个提案”）。工程里一般用它的扩展：**Multi-Paxos** 来做一串值（日志）。
Paxos 的核心角色：
* **Proposer**：提出提案的人
* **Acceptor**：投票/承诺的人（关键，决定共识）
* **Learner**：学习最终决定的人（知道结果即可）

### 单值 Paxos 的两阶段（最经典那版）
**Phase 1：Prepare / Promise**
* Proposer 选择一个全局唯一且递增的编号 n（proposal number），发 Prepare(n) 给 acceptors。
* Acceptor 如果没承诺过更大的编号，就回复 Promise：

  * 我承诺以后不再接受编号 < n 的提案
  * 并告诉你我曾经接受过的最高编号提案（如果有）

**Phase 2：Accept Request / Accepted**
* Proposer 收到多数 Promise 后，决定提案值 v：

  * 若 Promise 里有人报告曾接受过某个值，就必须沿用其中**编号最大**的那个值（这是安全性关键）。
  * 否则可以用自己原本想提的 v。
* 然后发 Accept(n, v) 给 acceptors。
* Acceptor 若没承诺过更大的编号，就接受并回复 Accepted。
* 多数 acceptors 接受后，这个值就达成共识。

> 我理解raft和paxos的区别在于，raft是每个结点有自己的身份，选出一个领导者，后面一切结论以领导者为准，paxos是一个结点可能有多个身份，每个结论是结点间经过协商投票得出的，最后交由所有结点执行

> Raft 是强 leader 的复制日志协议：节点有明确的 follower/candidate/leader 状态，写入只能经 leader 排序；但 entry 是否提交取决于多数派复制，leader 并不是独裁者。
> Paxos 的角色是 proposer/acceptor/learner，一个节点可以同时承担多个角色。每个值是否被选定由 acceptor 的多数派 quorum 决定，其他节点作为 learner 学到结果后再应用到状态机；工程化的 Multi-Paxos 通常也会引入 leader proposer 来减少反复的 prepare，但它的“leader”是优化层而不是像 Raft 那样的显式三态机。


# 介绍一下中断上下文机制(上半部，下半部，workqueue/softirq/tasklet)
http://www.wowotech.net/irq_subsystem/interrupt_subsystem_architecture.html


# 介绍一下dm-pcache


# 介绍一下samba








