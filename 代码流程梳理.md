# 下IO流程(无调度器)

```
原始请求 --> bio --> request

ksys_read
 vfs_read
  new_sync_read
   call_read_iter
    ext4_file_read_iter // file->f_op->read_iter
	 ext4_dio_read_iter
	  iomap_dio_rw
	   __iomap_dio_rw
	    blk_start_plug
		iomap_apply
		 ext4_iomap_begin // ops->iomap_begin
		 iomap_dio_actor
		  iomap_dio_bio_actor
		   bio_alloc // 分配bio
		   bio_set_dev // 根据iomap->bdev(inode->i_sb->s_bdev)设置 bi_disk/bi_partno
		   iomap_sector // 设置设备物理地址
		   bio_iov_iter_get_pages // 目标内存地址
		    __bio_iov_iter_get_pages
			 iov_iter_get_pages
			 __bio_add_page
			  // bio->bi_iter.bi_size += len
		   iomap_dio_submit_bio
		    submit_bio
			 submit_bio_noacct
			  __submit_bio_noacct
			   __submit_bio
			    <-------------------- 没有 .submit_bio 回调 -------------------->
			    blk_mq_submit_bio
				 __blk_mq_alloc_request // 分配 request
				  blk_mq_get_ctx // 获取ctx per_cpu变量
				   __blk_mq_get_ctx
				    // per_cpu_ptr(q->queue_ctx, cpu)
				  blk_mq_map_queue // 获取 hctx
				   // ctx->hctxs[type]
				  blk_mq_get_tag // 拿tag
				   __blk_mq_get_tag
				    __sbitmap_queue_get
					 sbitmap_get
					  sbitmap_find_bit_in_index // index : 第几个word; alloc_hint : word内偏移
				  blk_mq_rq_ctx_init // 初始化 request
				   blk_mq_tags_from_data // 获取tags 调度器sched_tags或者硬件tags
				   // tags->static_rqs[tag] 根据拿到的tag从static_rqs中获取 request
				 blk_mq_bio_to_request // 使用bio填充request
				 blk_mq_plug // 获取current->plug
				 blk_add_rq_to_plug
				  // list_add_tail(&rq->queuelist, &plug->mq_list) 将request插入plug->mq_list
				<-------------------- 有 .submit_bio 回调 -------------------->
				dm_submit_bio
				 __split_and_process_bio
				  __split_and_process_non_flush
				   dm_table_find_target // 查找目标设备
				   __clone_and_map_data_bio
				    alloc_tio // 分配bio
					clone_bio // 拷贝原始bio中的一些信息
					__map_bio // 映射实际物理设备
					 linear_map // ti->type->map
					  linear_map_bio
					   bio_set_dev // 设置实际物理设备的 bd_disk bd_partno
					 submit_bio_noacct // 提交clone后的bio(实际物理设备的bio)
		 ext4_iomap_end // ops->iomap_end
		blk_finish_plug
		 blk_flush_plug_list
		  blk_mq_flush_plug_list
		   blk_mq_sched_insert_requests
		    blk_mq_insert_requests
			 list_splice_tail_init // 将request插入ctx->rq_lists[hctx->type]
			blk_mq_run_hw_queue
			 __blk_mq_delay_run_hw_queue
			  blk_mq_sched_dispatch_requests
			   __blk_mq_sched_dispatch_requests
			    blk_mq_flush_busy_ctxs // 从ctx->rq_lists[hctx->type]暂存到局部链表 rq_list 中
				 sbitmap_for_each_set
				  __sbitmap_for_each_set
				   flush_busy_ctx
				    // list_splice_tail_init(&ctx->rq_lists[type], flush_data->list) request暂存到 rq_list 中
			    blk_mq_dispatch_rq_list
				 blk_mq_prep_dispatch_rq
				  blk_mq_get_driver_tag
				   // hctx->tags->rqs[rq->tag] = rq; 实际使用的tag
				 scsi_queue_rq // q->mq_ops->queue_rq
				  blk_mq_start_request
				   blk_add_timer
				  scsi_dispatch_cmd
				   // 将 request 提交到具体驱动


// 驱动完成 request 后调 end_io
blk_mq_end_request
 blk_update_request
  req_bio_endio
   bio_endio
    iomap_dio_bio_end_io // bio->bi_end_io
```

# 初始化queue/tag set流程

```
virtscsi_probe
 scsi_add_host
  scsi_add_host_with_dma
   scsi_mq_setup_tags // blk_mq_tag_set Scsi_Host->tag_set
    blk_mq_alloc_tag_set
     blk_mq_alloc_set_map_and_rqs
      __blk_mq_alloc_rq_maps
       __blk_mq_alloc_map_and_rqs // 每个硬件队列执行一次
        blk_mq_alloc_map_and_rqs // 初始化 blk_mq_tag_set->tags[hctx_idx]
         blk_mq_alloc_rq_map // 分配 blk_mq_tags
          blk_mq_init_tags // 分配初始化 blk_mq_tags
           blk_mq_init_bitmaps // 初始化 blk_mq_tags->bitmap_tags
          kcalloc_node // 分配 blk_mq_tags->rqs blk_mq_tags->static_rqs
 scsi_scan_host
  do_scan_async
   do_scsi_scan_host
    scsi_scan_host_selected
     scsi_scan_channel
      __scsi_scan_target
       scsi_probe_and_add_lun
        scsi_alloc_sdev
         scsi_mq_alloc_queue // 分配 request_queue
          blk_mq_init_queue // 使用 blk_mq_tag_set 分配初始化 request_queue
           blk_mq_init_queue_data
            blk_alloc_queue // 分配 request_queue
            blk_mq_init_allocated_queue // 初始化 request_queue
         scsi_sysfs_device_initialize
          device_initialize // 初始化 device

really_probe
 sd_probe
  alloc_disk // 分配 gendisk
  device_add_disk

// scsi_mq_setup_tags
blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 blk_mq_realloc_tag_set_tags
 blk_mq_update_queue_map
 blk_mq_alloc_map_and_requests
  __blk_mq_alloc_rq_maps
   __blk_mq_alloc_map_and_request
    blk_mq_alloc_rq_map // 为每个硬件队列分配 blk_mq_tags
	 blk_mq_init_tags
	  kzalloc_node // 分配 blk_mq_tags
	  blk_mq_init_bitmap_tags
	   bt_alloc // 分配 bitmap_tags
	   bt_alloc // 分配 breserved_tags
	    sbitmap_queue_init_node
		 sbitmap_init_node
		  // sb->map = kcalloc_node(sb->map_nr, sizeof(*sb->map), flags, node);
	   // tags->bitmap_tags = &tags->__bitmap_tags;
	   // tags->breserved_tags = &tags->__breserved_tags;


// scsi_mq_alloc_queue
blk_mq_init_queue(struct blk_mq_tag_set *set) // 不带调度器
 blk_mq_init_queue_data
  blk_alloc_queue // 分配 request_queue，简单初始化
   bdi_alloc // 分配 bdi
   // q->timeout --> blk_rq_timed_out_timer
   // q->timeout_work --> blk_timeout_work
  blk_mq_init_allocated_queue // 初始化 request_queue
   blk_mq_alloc_ctxs // 分配 blk_mq_ctxs blk_mq_ctx
   blk_mq_sysfs_init
   blk_mq_realloc_hw_ctxs
    kcalloc_node // 分配 blk_mq_hw_ctx 指针
	blk_mq_alloc_and_init_hctx // 分配 blk_mq_hw_ctx
	 blk_mq_alloc_hctx // 分配
	  kmalloc_array_node // 分配 blk_mq_ctx 指针
	 blk_mq_init_hctx // 初始化
	  // hctx->tags = set->tags[hctx_idx] 初始化 blk_mq_tags
	  blk_mq_init_request
   // q->timeout_work --> blk_mq_timeout_work
   blk_mq_init_cpu_queues
   blk_mq_add_queue_tag_set
   blk_mq_map_swqueue
    // ctx = per_cpu_ptr(q->queue_ctx, i)
    // hctx->ctxs[hctx->nr_ctx++] = ctx


device_add_disk
 __device_add_disk
  elevator_init_mq
   blk_mq_init_sched
    blk_mq_sched_alloc_tags
	 blk_mq_alloc_rq_map // 分配 sched_tags
```

# 删盘流程

```
pci_device_remove
 nvme_remove
  nvme_remove_namespaces
   nvme_ns_remove
    del_gendisk
	 device_del
	  devtmpfs_delete_node
	   devtmpfs_submit_req
    nvme_put_ns
	 put_disk
	  kobject_put
	   kobject_release
	    kobject_cleanup
		 device_release // t->release
		  disk_release // dev->type->release
		   blk_put_queue
		    blk_release_queue
			 blk_exit_queue
			  bdi_put

[  100.444778] CPU: 1 PID: 31 Comm: kdevtmpfs Not tainted 5.10.0-04314-ga3d351e33f38-dirty #84
[  100.447657] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS ?-20190727_073836-buildvm-ppc64le-16.ppc.fedoraproject.org-3.fc31 04/01/2014
[  100.452406] Call Trace:
[  100.453185]  dump_stack+0xa6/0xd5
[  100.454203]  bdev_evict_inode+0x15/0x169
[  100.455377]  evict+0x10c/0x320
[  100.456310]  iput+0x1fa/0x390
[  100.457176]  bd_forget+0x74/0xa0
[  100.458174]  evict+0x1fb/0x320
[  100.459227]  iput+0x1fa/0x390
[  100.460144]  vfs_unlink+0x2c9/0x3c0
[  100.462658]  handle_remove+0x197/0x37f
[  100.465589]  devtmpfs_work_loop.cold+0x15/0x24
[  100.468920]  devtmpfsd+0xba/0xd3
[  100.471333]  ? clkdev_alloc+0x9d/0x9d
[  100.473818]  kthread+0x153/0x1b0
[  100.476070]  ? kthread_flush_work+0x170/0x170
[  100.480105]  ret_from_fork+0x1f/0x30

iput
 iput_final
  evict
   bdev_evict_inode // op->evict_inode
    bdi_put
   destroy_inode
    i_callback
     bdev_free_inode // inode->free_inode
```

# bdi生命周期

```
queue初始化
blk_alloc_queue
 bdi_alloc
  bdi_init
   kref_init

queue释放
blk_release_queue
 blk_exit_queue
  bdi_put

块设备inode初始化
__blkdev_get
 bdi_get

块设备inode释放
evict
 bdev_evict_inode
  bdi_put
```

# dm thin-pool查找分配块

```c
// 初始化worker
pool_ctr
 __pool_find
  pool_create
   // INIT_WORK(&pool->worker, do_worker);

// 唤醒worker
pool_resume
 do_waker
  wake_worker
   // queue_work(pool->wq, &pool->worker);

// worker执行流程
do_worker
 process_deferred_bios
  get_first_thin // 从pool->active_thins中获取thin，thin设备通过thin_ctr将自身添加到链表中
  process_thin_deferred_bios // 处理当前thin设备上的bio
   bio_list_merge // 将tc->deferred_bio_list存到局部变量，同时清空tc->deferred_bio_list
   process_bio // pool->process_bio 取出第一个bio进行处理
    get_bio_block // 根据bio的sector计算对应的block
	build_virtual_key // 使用目标 逻辑 block 初始化cell key
	bio_detain // drivers\md\dm-thin.c
	 dm_bio_prison_alloc_cell // 从prison的mempool里分配cell
	 dm_bio_detain
	  bio_detain // drivers\md\dm-bio-prison-v1.c
	   __bio_detain // 从prison->cells的红黑树里查找key对应的cell是否存在，如果存在，则使用已有的cell，否则使用新分配的cell
	    __setup_new_cell // 初始化新分配的cell，将cell与key/bio关联起来(cell->key/cell->holder)
	process_cell // 处理新分配的cell

process_cell
 dm_thin_find_block // 根据逻辑块号，找到对应的信息(物理块号及共享信息)
  __find_block // 两级索引，第一级索引key为thin id，第二级索引key为逻辑块号
   dm_btree_lookup
	// 第一次循环，查找目标thin的映射信息所在块；第二次循环，查找目标thin上逻辑块与物理块的对应关系
	btree_lookup_raw
	 ro_step // 将new_child对应的数据读取到s->nodes + s->count对应的buffer中 --> struct btree_node
	 ro_node // 获取s->nodes[s->count - 1]对应的buffer --> struct btree_node
	 lower_bound // search_fn 在btree_node中二分查找指定的key，返回对应的索引值
	 // 如果是中间结点，则根据刚刚获取到的索引值，查找下一级结点所在的block
	 // 返回查找到的key值rkey与value值value_p
   unpack_lookup_result
	// 从查找到的value值中解析出物理块号exception_block和时间exception_time(根据time判断是否共享)
 process_shared_bio # 如果该块是多个thin设备共享的块号，需要进行处理
  build_data_key // 使用目标 物理 block 初始化cell key
   break_sharing
     alloc_data_block # 返回 -EINVAL
       dm_pool_alloc_data_block # 返回 -EINVAL
         dm_sm_new_block // 新分配一个块，用于data
		 // pmd->data_sm
		 // container_of(sm, struct sm_disk, sm)
		 // smd->old_ll
           sm_disk_new_block # 通过 sm->new_block 回调
             sm_ll_find_free_block // 计算bitmap的逻辑块范围，根据逻辑块号依次读取存放bitmap信息的物理块，根据bitmap信息查找空闲块
			  // 遍历bitmap的所有块，每个块上包含ll->entries_per_block个entry，表示块的使用情况
			  disk_ll_load_ie // ll->load_ie 查找bitmap逻辑块i对应的 disk_index_entry ，存在ie_disk中
			  dm_tm_read_lock //  根据 disk_index_entry 判断，有空闲块，查找物理块ie_disk.blocknr，获取详细的bitmap信息
			  sm_find_free // 查找空闲块
			  // 计算空闲块并返回 *result = i * ll->entries_per_block + (dm_block_t) position;
             sm_ll_inc
               sm_ll_mutate(inc_ref_count)
                disk_ll_load_ie // ll->load_ie
                dm_tm_shadow_block # 此处返回错误 -EINVAL
                 dm_sm_count_is_more_than_one
                  sm_disk_count_is_more_than_one // sm->count_is_more_than_one 判断引用计数是否大于1
                   sm_disk_get_count // 获取引用计数
                	sm_ll_lookup
                	 sm_ll_lookup_bitmap // 引用计数小于3，则从bitmap中获取
                	 sm_ll_lookup_big_ref_count // 引用计数大于等于3，则从引用计数信息保存块上获取
                	  dm_btree_lookup
                 __shadow_block
                  dm_sm_new_block # 新分配一个块， 用于metadata
				  // smd->ll
				  // ll->tm
				  // tm->sm
				  // container_of(sm, struct sm_metadata, sm)
				  // smm->old_ll
				   sm_metadata_new_block
				    sm_metadata_new_block_
					 sm_ll_find_free_block
                  dm_sm_dec_block # 原始块的引用计数 -1
                   sm_metadata_dec_block # 通过 sm->dec_block 回调
                    sm_ll_dec
                     sm_ll_mutate(dec_ref_count)
                      dec_ref_count
                       DMERR_LIMIT("unable to decrement a reference count below 0");
                  dm_bm_read_lock # 从原始块中读取数据
                  dm_bm_write_lock_zero # 新块中的数据清零
                  memcpy # 将老数据复制到新块中
                DMERR("dm_tm_shadow_block() failed");
      metadata_operation_failed(dm_pool_alloc_data_block)
       DMERR_LIMIT("%s: metadata operation '%s' failed: error = %d")
        abort_transaction
         DMERR_LIMIT("%s: aborting current metadata transaction", dev_name);
        set_pool_mode(PM_READ_ONLY)
         notify_of_pool_mode_change(read-only)
          DMINFO("%s: switching pool to %s mode")
    DMERR_LIMIT("%s: alloc_data_block() failed: error = %d",)

table_load
 populate_table
  dm_table_add_target
   dm_get_target_type // 根据字符串获取 target_type 用于赋值给tgt->type
    get_target_type
	 __find_target_type
	  // 根据传入的name参数寻找已注册的 target_type，thin 对应 thin_target
	  // thin设备将关联 thin_target，可以使用.ctr函数 thin_ctr
   thin_ctr // tgt->type->ctr

修改元数据块时，需要获取一个新的空闲元数据块，作为待修改块的shadow1。
获取用于作为shadow1的空闲元数据块，理论上需要修改该块的bitmap，即该块bitmap信息所在的块，也需要一个shadow2，这样会陷入无限递归。

要防止无限递归，需要做到两点：
1、获取作为shadow2的空闲块时，不会立刻修改shadow2块的bitmap，使得shadow2块在不修改bitmap的情况下可以使用
2、shadow2块在不修改bitmap的情况下使用时，要防止其他进程作为空闲块获取使用

解决方案：
1、获取shadow2块后不修改shadow2的bitmap，将修改请求加入等待队列
2、获取到一个空闲块后增加smm->begin，保证后一次查找空闲块的范围不包括前一次查找到的空闲块

dm_pool_alloc_data_block // 分配新的数据块
 dm_sm_new_block
  sm_disk_new_block
   sm_ll_find_free_block // 查找到空闲数据块
   sm_ll_inc // 修改空闲数据块的bitmap信息，增加空闲数据块的引用计数
    sm_ll_mutate
     dm_tm_shadow_block // 为保留新分配的空闲块bitmap信息的物理block分配shadow1
      __shadow_block
       dm_sm_new_block
        sm_metadata_new_block
         sm_metadata_new_block_
          sm_ll_find_free_block // 查找空闲元数据块shadow1
          smm->begin = *b + 1; // 更新begin，保证下一次调用该函数查找空闲块时，及时上次查找到的已经在使用的空闲块没有更新bitmap，也不会第二次被获取使用
          in(smm)
          sm_ll_inc // 修改空闲元数据块的bitmap信息，增加空闲元数据块的引用计数 shadow1
           sm_ll_mutate
            dm_tm_shadow_block
             __shadow_block
              dm_sm_new_block
               sm_metadata_new_block
                sm_metadata_new_block_
                 sm_ll_find_free_block // 查找空闲元数据块shadow2(由于begin已更新，shadow1和shadow2不会是同一个块)
                  recursing // 判断出当前正在shadow1的sm_ll_inc流程中
                  add_bop // 将对shadow2的sm_ll_inc操作将入等待队列
                  // 此时shadow2可供使用(bitmap更新操作延迟)，shadow1的sm_ll_inc流程可继续进行
          out(mm) // shadow1的sm_ll_inc流程完成后，会将等待队列中的请求取出执行

// 每次更新一个shadow的bitmap，一直更新到superblock中的metadata_space_map_root
```

# 切换调度器

```
elevator_switch
 blk_mq_freeze_queue
  blk_freeze_queue_start
   percpu_ref_kill // q->q_usage_counter
   blk_mq_run_hw_queues
  blk_mq_freeze_queue_wait
   wait_event
 blk_mq_quiesce_queue
  blk_mq_quiesce_queue_nowait
   blk_queue_flag_set // QUEUE_FLAG_QUIESCED
  synchronize_srcu
 elevator_switch_mq
 blk_mq_unquiesce_queue
  blk_queue_flag_clear // QUEUE_FLAG_QUIESCED
  blk_mq_run_hw_queues
 blk_mq_unfreeze_queue
```

# dm-thin-pool discard bio 处理流程

```c
man blkdiscard
// 下发 discard IO 流程
blk_ioctl_discard
 blkdev_issue_discard
  __blkdev_issue_discard
   bio_discard_limit // 起始位置是否按粒度对齐，如果不对齐，则返回起始位置与下一个对齐位置的差，否则返回粒度对齐的最大长度（这里用于保证起始位置对齐）
   blk_next_bio // REQ_OP_DISCARD
    submit_bio
	 submit_bio_noacct
	  submit_bio_noacct_nocheck
	   __submit_bio_noacct
	    __submit_bio
		 .submit_bio

dm_submit_bio
 dm_split_and_process_bio
  bio_split_to_limits
   __bio_split_to_limits
    bio_split_discard
	 bio_split // 拆分出 split_sectors 个sector，保证 当前sector偏移+split_sectors按granularity粒度对齐，且不超过sector偏移+max_discard_sectors
	  bio_alloc_clone // clone一个bio，bi_size更新为需要split的size，并在原始bio中减去这部分size
	bio_chain
	 // bio->bi_private = parent 所有递归提交的bio，parent均为原始bio，要等原始bio完成，整个IO才算全部完成
	submit_bio_noacct // 递归提交剩余部分
  init_clone_info
  __split_and_process_bio
   __process_abnormal_io
    __send_changing_extent_only
	 __max_io_len
	  dm_target_offset // 整个dm设备的offset对应当前target上的offset
	  max_io_len_target_boundary // 当前target上要操作的len
	 __send_duplicate_bios
	  alloc_tio
	  __map_bio
	   thin_map
	    thin_bio_map
		 thin_defer_bio_with_throttle
		  thin_defer_bio
		   // deferred_bio_list
  bio_trim // 如果上面流程提交的bio已经覆盖了所有范围，则trim后bio的size为0
  submit_bio_noacct


wake_worker
 do_worker
  process_deferred_bios
   process_thin_deferred_bios
    process_discard_bio
	 get_bio_block_range // 将范围单位由sector转换成block
	 build_key // dm_thin_device 在给pool传递 create_thin message时创建，在 create thin 时使用，与 thin_c 关联
	  dm_cell_key_has_valid_range // 不能超过 BIO_PRISON_MAX_RANGE_SHIFT 即1024个block
	 bio_detain
	  dm_bio_prison_alloc_cell // 预分配 new cell
	  dm_bio_detain
	   bio_detain
	    __bio_detain // 查找红黑树，当前bio插入 old cell 或者初始化 new cell
	  dm_bio_prison_free_cell // 根据是否使用了 new cell 决定是否释放 new cell
/*
 * prison + cell 暂存discard bio，短期内的多个相同的discard只会下发一次
 */

若bio插入 old cell，则等下一次调用处理：
process_deferred_bios
 process_thin_deferred_cells
  pool->process_discard_cell

若bio插入 new cell，则立刻处理：
pool->process_discard_cell
process_discard_cell_no_passdown
 pool->process_prepared_discard
 process_prepared_discard_passdown_pt1
  dm_thin_remove_range
   __remove_range
    dm_btree_lookup // 在 data_mapping_root 上根据 thin_id 查找 thin[id] 映射信息所在块，保存了thin逻辑块到pool逻辑块的映射关系
	dm_tm_inc // 增加 mapping_root 块的引用计数
	dm_btree_remove // 从 data_mapping_root 上删除 thin_id 对应的索引信息
	                // 只删除索引，thin[id] 映射信息所在块仍存在
	 remove_raw // 在btree中分层逐级删除 thin_id 的索引信息直到叶子节点，返回叶子节点上 thin[id] 映射信息所在块
	 delete_at // 删除叶子节点上 thin[id] 映射信息所在块
	dm_btree_lookup_next // 期望从 begin 开始删除，但用户态传递的 begin 并不一定已经 map ，删除要从已map的块开始
	                     // 从传入的begin开始，检索下一个在key[]中，即已map的块，赋回begin
						 // value中保存更新后的begin对应的value
	 dm_btree_lookup_next_single
	dm_btree_remove_leaves // 删除当前thin设备数据映射信息中指定范围的块映射信息
	 remove_one
	  remove_nearest // 查找到最近要删除的块
	  delete_at // 删除块信息
	dm_btree_insert // 将新的键值对 thinid--mapping_root 插入 data_mapping_root 对应的树中


discard之后是清数据还是标记块？ ———— 解除thin数据映射信息中相应块的映射
discard之后解除映射的块未标成空闲？ ———— 有什么地方减引用计数？(涛哥说会标成空闲)
读一个未分配的块？ ———— 分配空闲块，返回0
    
// 下发 discard bio
dm_submit_bio
 __split_and_process_bio
  __split_and_process_non_flush
   __clone_and_map_data_bio
    __map_bio
	 thin_map
	  thin_bio_map
	   thin_defer_bio_with_throttle
	    thin_defer_bio
		 bio_list_add // 加入链表
		 wake_worker // 唤醒 worker

// 处理 discard bio
do_worker // pool->worker
 process_deferred_bios
  process_thin_deferred_bios
   process_discard_bio // pool->process_discard
    get_bio_block_range // 计算 discard 范围，若小于一个 block，则结束 discard bio
	tc->pool->process_discard_cell // 继续处理 discard bio
```

# truncate 打开文件流程

```c
open
 do_sys_open
  do_sys_openat2
   build_open_flags
    // op->open_flag = flags;
    //	if (flags & O_TRUNC)
    //		acc_mode |= MAY_WRITE;
    // op->acc_mode = acc_mode
   do_filp_open
    path_openat
	 alloc_empty_file // op->open_flag
	  __alloc_file // f->f_flags = flags = op->open_flag
     do_open
	  handle_truncate // truncate 成长度为0
	   do_truncate
	    notify_change
		 ext4_setattr
		  ext4_truncate
		   ext4_discard_preallocations
		   ext4_ext_truncate
		    ext4_es_remove_extent // 删除 extent
		    ext4_ext_remove_space
			 ext4_ext_rm_leaf
			  ext4_remove_blocks
			   ext4_free_blocks
			    ext4_issue_discard // test_opt(sb, DISCARD)
				 sb_issue_discard
				  blkdev_issue_discard
				   __blkdev_issue_discard
				   submit_bio_wait // 下发 discard
```

