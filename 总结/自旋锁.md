# 一、疑问

1、什么是自旋锁

2、怎么拿自旋锁

3、怎样算拿到自旋锁

4、怎么释放自旋锁

5、自旋锁怎么来的

# 二、自旋锁历史



![](D:\4-oskernel\技术分享\自旋锁介绍\自旋锁发展史.png)

自旋锁：加锁失败则忙等待

为啥要用自旋锁？
对于临界区小(为啥能自旋)的重要流程，需尽快拿到临界资源执行重要操作(为啥要自旋)

为啥要自旋等待？
需要尽快拿到临界资源；针对队列自旋锁，在锁释放的情况下，如果前面的等待者调度出去长时间不拿锁，后面的所有等待者都会被阻塞

## 1、原始自旋锁

**一个CPU占用，多个CPU等锁，占用者释放后等待者随机抢占锁**

```c
typedef struct {
	raw_spinlock_t raw_lock;
} spinlock_t;

typedef struct {
	unsigned int slock;
} raw_spinlock_t;
```

**原始自旋锁用1来表示没有加锁**

### 加锁：

关抢占不关中断（允许中断，但中断返回后不允许抢占）

(为什么不关中断？中断执行较快，对临界资源的使用效率影响较小)

```c
// 加锁动作由汇编实现，这里转换成C语言逻辑
// 无人持锁时 slock 1-->0，加锁成功
// 有人持锁时
// 第一个等锁者 slock 0-->-1 循环等待slock置1
// 后续等锁者  slock 任意负值-->任意负值-1 循环等待slock置1
// slock置1后各等锁者抢占
static inline void __raw_spin_lock(raw_spinlock_t *lock)
{
    while(1){
        if(--lock->slock == 0) // 汇编代码中有lock指令前缀，此操作是原子的
            return;
        while((int)lock->slock <= 0) {}
    }
}
```

### 解锁：

```c
// slock 0或任意负值 --> 1
static inline void __raw_spin_unlock(raw_spinlock_t *lock)
{
    asm volatile("movb $1,%0" : "+m" (lock->slock) :: "memory");
}
```

解锁操作就是直接将slock置1，无论是否有其他进程在等锁，有多少进程在等锁，之后由各等锁进程随机抢锁



## 2、票号自旋锁

**增加排队机制**

票号自旋锁用head == tail表示没有加锁，初始为{head:0,tail:0}状态



![](D:\4-oskernel\技术分享\自旋锁介绍\票号自旋锁.png)



```c
typedef struct spinlock {
    struct raw_spinlock rlock;
} spinlock_t;

typedef struct raw_spinlock {
    arch_spinlock_t raw_lock;
} raw_spinlock_t;

typedef struct arch_spinlock {
    union {
        __ticketpair_t head_tail;
        struct __raw_tickets {
            __ticket_t head, tail;
        } tickets;
    };
} arch_spinlock_t;

#if (CONFIG_NR_CPUS < (256 / __TICKET_LOCK_INC))
typedef u8  __ticket_t;
typedef u16 __ticketpair_t;
#else
typedef u16 __ticket_t;
typedef u32 __ticketpair_t;
#endif
```

### 加锁：

关抢占不关中断

```c
static __always_inline void arch_spin_lock(arch_spinlock_t *lock)
{
    register struct __raw_tickets inc = { .tail = TICKET_LOCK_INC };

    inc = xadd(&lock->tickets, inc); // 拿到tail值作为自己的ticket，并将tail加1
    if (likely(inc.head == inc.tail)) // 比较head是否等于ticket，若等于，则加锁成功(tail为加1前的值，即ticket)
        goto out;

    for (;;) {
        unsigned count = SPIN_THRESHOLD;

      do {
          inc.head = READ_ONCE(lock->tickets.head); // 加锁失败，则不断读取head，直到head等于之前的tail(即ticket)，加锁成功
          if (__tickets_equal(inc.head, inc.tail))
              goto clear_slowpath;
          cpu_relax();
      } while (--count);
      __ticket_lock_spinning(lock, inc.tail);
    }
clear_slowpath:
    __ticket_check_and_clear_slowpath(lock, inc.head);
out:
    barrier();  /* make sure nothing creeps before the lock is taken */
}
```

### 解锁：

将head值加1

```
static __always_inline void arch_spin_unlock(arch_spinlock_t *lock)
{
    __add(&lock->tickets.head, TICKET_LOCK_INC, UNLOCK_LOCK_PREFIX);
}
```



## 3、MCS自旋锁

**使用队列，解决cache颠簸，提升性能**

传统自旋锁有一个很大的性能问题，所有等待同一个自旋锁的CPU在同一个变量上自旋等待，获得或者释放锁的时候会对这个变量进行修改。对于单CPU的系统，这个不是问题，但是对于SMP多CPU系统来说，由于缓存一致性的问题，一个CPU写入了一个变量后，必须要让所有其它处理器上对应该变量的缓存行失效，还需要使用内存屏障，在拥有几百甚至几千个处理器的大型系统中，将导致系统性能大幅下降。

[cache颠簸]: https://carlyleliu.github.io/2021/Linux%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%EF%BC%88%E4%B8%80%EF%BC%89Cache%E5%8E%9F%E7%90%86/

mcs_spinlock类型的指针为NULL表示没有加锁



![](D:\4-oskernel\技术分享\自旋锁介绍\MCS自旋锁-1.png)



![](D:\4-oskernel\技术分享\自旋锁介绍\MCS自旋锁-2.png)



```c
struct mcs_spinlock {
	struct mcs_spinlock *next;
	int locked; /* 1 if lock acquired */
};
```



```
lock				ProcessA			ProcessB			ProcessC
++++++++++++
+ lock     +
+ --> NULL +
+ locked   +
+ --> 0    +
++++++++++++

<-------------------ProcessA lock------------------->

					++++++++++++
					+ lock     +
					+ --> NULL +
					+ locked   +
					+ --> 0    +
					++++++++++++
						nodeA

++++++++++++
+ lock     +
+ --> nodeA+
+ locked   +
+ --> 0    +
++++++++++++

										++++++++++++
										+ lock     +
										+ --> NULL +
										+ locked   +
										+ --> 0    +
										++++++++++++
											nodeB

++++++++++++		++++++++++++		++++++++++++
+ lock     +		+ lock     +		+ lock     +
+ --> nodeB+		+ --> nodeB+		+ --> NULL +
+ locked   +		+ locked   +		+ [locked] +
+ --> 0    +		+ --> 0    +		+ --> 0    +
++++++++++++		++++++++++++		++++++++++++
						nodeA				nodeB

															++++++++++++
															+ lock     +
															+ --> NULL +
															+ locked   +
															+ --> 0    +
															++++++++++++
																nodeC

++++++++++++		++++++++++++		++++++++++++		++++++++++++
+ lock     +		+ lock     +		+ lock     +		+ lock     +
+ --> nodeC+		+ --> nodeB+		+ --> nodeC+		+ --> NULL +
+ locked   +		+ locked   +		+ [locked] +		+ [locked] +
+ --> 0    +		+ --> 0    +		+ --> 0    +		+ --> 0    +
++++++++++++		++++++++++++		++++++++++++		++++++++++++
						nodeA				nodeB				nodeC

<-------------------ProcessA unlock------------------->

++++++++++++		++++++++++++		++++++++++++		++++++++++++
+ lock     +		+ lock     +		+ lock     +		+ lock     +
+ --> nodeC+		+ --> nodeB+		+ --> nodeC+		+ --> NULL +
+ locked   +		+ locked   +		+ [locked] +		+ [locked] +
+ --> 0    +		+ --> 0    +		+ --> 1    +		+ --> 0    +
++++++++++++		++++++++++++		++++++++++++		++++++++++++
						nodeA				nodeB				nodeC

<-------------------ProcessB lock------------------->

++++++++++++		++++++++++++		++++++++++++		++++++++++++
+ lock     +		+ lock     +		+ lock     +		+ lock     +
+ --> nodeC+		+ --> nodeB+		+ --> nodeC+		+ --> NULL +
+ locked   +		+ locked   +		+ [locked] +		+ [locked] +
+ --> 0    +		+ --> 0    +		+ --> 1    +		+ --> 0    +
++++++++++++		++++++++++++		++++++++++++		++++++++++++
						nodeA				nodeB				nodeC

<-------------------ProcessB unlock------------------->

++++++++++++		++++++++++++		++++++++++++		++++++++++++
+ lock     +		+ lock     +		+ lock     +		+ lock     +
+ --> nodeC+		+ --> nodeB+		+ --> nodeC+		+ --> NULL +
+ locked   +		+ locked   +		+ [locked] +		+ [locked] +
+ --> 0    +		+ --> 0    +		+ --> 1    +		+ --> 1    +
++++++++++++		++++++++++++		++++++++++++		++++++++++++
						nodeA				nodeB				nodeC

<-------------------ProcessC lock------------------->

++++++++++++		++++++++++++		++++++++++++		++++++++++++
+ lock     +		+ lock     +		+ lock     +		+ lock     +
+ --> nodeC+		+ --> nodeB+		+ --> nodeC+		+ --> NULL +
+ locked   +		+ locked   +		+ [locked] +		+ [locked] +
+ --> 0    +		+ --> 0    +		+ --> 1    +		+ --> 1    +
++++++++++++		++++++++++++		++++++++++++		++++++++++++
						nodeA				nodeB				nodeC

<-------------------ProcessC unlock------------------->

++++++++++++		++++++++++++		++++++++++++		++++++++++++
+ lock     +		+ lock     +		+ lock     +		+ lock     +
+ --> NULL +		+ --> nodeB+		+ --> nodeC+		+ --> NULL +
+ locked   +		+ locked   +		+ [locked] +		+ [locked] +
+ --> 0    +		+ --> 0    +		+ --> 1    +		+ --> 1    +
++++++++++++		++++++++++++		++++++++++++		++++++++++++
						nodeA				nodeB				nodeC

++++++++++++
+ lock     +
+ --> NULL +
+ locked   +
+ --> 0    +
++++++++++++

```



### 加锁：

```c
static inline
void mcs_spin_lock(struct mcs_spinlock **lock, struct mcs_spinlock *node)
{
    struct mcs_spinlock *prev;
    /* Init node */
    node->locked = 0;
    node->next   = NULL;

    prev = xchg(lock, node); // 锁指针指向当前分配的node，同时将原本锁指针的值保存在prev中
    // 如果之前锁指针为NULL，说明没人占锁，直接返回，继续访问临界区即可
    // 此时锁指针已指向当前节点，如果有新的进程要占用锁，会发现锁指针非NULL，不会直接返回访问临界区，会走后续流程；
    // 如果已有进程占了锁，在访问临界区，当前进程修改锁指针，并不影响已占锁进程的执行，只是将当前进程的加锁请求加在已占锁进程的后面
    if (likely(prev == NULL)) {
        return;
    }
    // prev非NULL，说明prev中保存了上一个请求加锁进程的node，这个进程可能正在访问临界区，也可能正在等待它的上一个进程解锁
    // 将prev->next指向当前node，表明等待prev对应的进程解锁
    WRITE_ONCE(prev->next, node);
    /* Wait until the lock holder passes the lock down. */
    arch_mcs_spin_lock_contended(&node->locked);
}
```

### 解锁：

```c
static inline
void mcs_spin_unlock(struct mcs_spinlock **lock, struct mcs_spinlock *node)
{
    // 读取当前node的next
    // 可能为NULL，表明没有进程在等锁
    // 可能非NULL，表明有进程在等当前进程解锁
    struct mcs_spinlock *next = READ_ONCE(node->next);

    if (likely(!next)) { // 没有进程在等锁
        // 没有进程在等锁，此时锁指针应该指向当前node，将锁指针置NULL，释放锁
        if (likely(cmpxchg(lock, node, NULL) == node))
            return;
        // 锁指针被修改了，已经不指向当前node了，说明在执行READ_ONCE(node->next)读取当前node的next之后，到检查锁指针状态，这之间有新的进程希望占锁，这个新进程已经改了锁指针，后续会改当前node的next指针，等当前node解锁。对于这种情况，当前node应等新进程修改完next，这样才能获取到新进程的node，才能在当前进程解锁后让新进程占锁
        /* Wait until the next pointer is set */
        while (!(next = READ_ONCE(node->next)))
            cpu_relax_lowlatency();
    }
    /* Pass lock to next waiter. */
    arch_mcs_spin_unlock_contended(&next->locked); // 将等锁进程的locked置位，表明该进程可以占锁了
}
```

## 4、队列自旋锁

**使用percpu buffer定义自旋锁的node**

解决锁体积太大与接口不兼容问题

locked/pending/tail

locked：表示当前锁是否被占用
pending：为1表示有进程在等锁，为0需要看tail状态
tail：表示等锁队列中最后一个希望持锁的进程



```c
typedef struct qspinlock {
	union {
		atomic_t val;
 
#ifdef __LITTLE_ENDIAN
		struct {
			u8	locked;
			u8	pending;
		};
		struct {
			u16	locked_pending;
			u16	tail;
		};
#else
		struct {
			u16	tail;
			u16	locked_pending;
		};
		struct {
			u8	reserved[2];
			u8	pending;
			u8	locked;
		};
#endif
	};
} arch_spinlock_t;

#define MAX_NODES	4
static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);
// 获得自旋锁时会禁止内核抢占，但不一定会禁止中断
// 4个node分别对应4中不同情况下spin_lock的队列模式
// 1：非中断；2：软中断；3：硬中断；4：不可屏蔽中断
```



![](D:\4-oskernel\技术分享\自旋锁介绍\qspinlock.png)



**进程状态**

1、持锁，占locked位
2、等锁，占pending位
3、等锁，在队列中(在队列中不抢pending，等待locked和pending同时为0)
	3.1 队列头(自旋locked和pending)
	3.1 非队列头(自旋自己的locked)



![](D:\4-oskernel\技术分享\自旋锁介绍\队列自旋锁-1.png)



![](D:\4-oskernel\技术分享\自旋锁介绍\队列自旋锁-2.png)



![](D:\4-oskernel\技术分享\自旋锁介绍\队列自旋锁-3.png)



![](D:\4-oskernel\技术分享\自旋锁介绍\队列自旋锁-4.png)



![](D:\4-oskernel\技术分享\自旋锁介绍\队列自旋锁-5.png)



```c
void __lockfunc queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
{
	struct mcs_spinlock *prev, *next, *node;
	u32 old, tail;
	int idx;

	BUILD_BUG_ON(CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS));

	if (pv_enabled())
		goto pv_queue;

	if (virt_spin_lock(lock))
		return;

	/*
	 * Wait for in-progress pending->locked hand-overs with a bounded
	 * number of spins so that we guarantee forward progress.
	 *
	 * 0,1,0 -> 0,0,1
	 */
	 // 锁已被释放，有CPU占据pending正在等待
	 // 等占据pending的CPU持锁并释放pending
	if (val == _Q_PENDING_VAL) {
		int cnt = _Q_PENDING_LOOPS;
		val = atomic_cond_read_relaxed(&lock->val,
					       (VAL != _Q_PENDING_VAL) || !cnt--);
	}

	/*
	 * If we observe any contention; queue.
	 */
	 // 检测到竞争，走队列模式
	 // 1. pending非0，有CPU抢到了pending
	 // 2. tail非0，有CPU没抢到pending走队列模式
	if (val & ~_Q_LOCKED_MASK)
		goto queue;

	/*
	 * trylock || pending
	 *
	 * 0,0,* -> 0,1,* -> 0,0,1 pending, trylock
	 */
	 // pending为0时直接抢pending
	 // 1. locked非0，有CPU持锁，当前CPU作为唯一等锁的CPU，等持锁CPU释放锁后持锁
	 // 2. locked为0，无CPU持锁，当前CPU释放pending，持锁
	val = queued_fetch_set_pending_acquire(lock);

	/*
	 * If we observe contention, there is a concurrent locker.
	 *
	 * Undo and queue; our setting of PENDING might have made the
	 * n,0,0 -> 0,0,0 transition fail and it will now be waiting
	 * on @next to become !NULL.
	 */
	 // 检测到竞争，抢pending失败
	 // 1. pending非0，有CPU已经抢到了pending，当前CPU抢失败，直接走队列模式
	 // 2. pending为0，tail非0，pending位应由tail对应的已经在等待队列里的CPU占有，当前CPU误抢pending，需要释放
	if (unlikely(val & ~_Q_LOCKED_MASK)) {

/*
还需要将设置的pending位再清空成0，不过为什么能直接这样操作，没有竞争吗？这是因为最多只会有一个CPU监测到这种pending位从0到1的跳变
*/
	 // 只有置pending位的CPU会清这次置位后的pending
		/* Undo PENDING if we set it. */
		if (!(val & _Q_PENDING_MASK))
			clear_pending(lock);

		goto queue;
	}

	/*
	 * We're pending, wait for the owner to go away.
	 *
	 * 0,1,1 -> 0,1,0
	 *
	 * this wait loop must be a load-acquire such that we match the
	 * store-release that clears the locked bit and create lock
	 * sequentiality; this is because not all
	 * clear_pending_set_locked() implementations imply full
	 * barriers.
	 */
	 // 已抢到pending，如果locked非0，有CPU已持锁，则在该锁上自旋等待
	if (val & _Q_LOCKED_MASK)
		atomic_cond_read_acquire(&lock->val, !(VAL & _Q_LOCKED_MASK));

	/*
	 * take ownership and clear the pending bit.
	 *
	 * 0,1,0 -> 0,0,1
	 */
	 // 当前CPU持锁
	clear_pending_set_locked(lock);
	lockevent_inc(lock_pending);
	return;

	/*
	 * End of pending bit optimistic spinning and beginning of MCS
	 * queuing.
	 */
queue:
	lockevent_inc(lock_slowpath);
pv_queue:
	node = this_cpu_ptr(&qnodes[0].mcs);
	idx = node->count++;
	tail = encode_tail(smp_processor_id(), idx);

	trace_contention_begin(lock, LCB_F_SPIN);

	/*
	 * 4 nodes are allocated based on the assumption that there will
	 * not be nested NMIs taking spinlocks. That may not be true in
	 * some architectures even though the chance of needing more than
	 * 4 nodes will still be extremely unlikely. When that happens,
	 * we fall back to spinning on the lock directly without using
	 * any MCS node. This is not the most elegant solution, but is
	 * simple enough.
	 */
	if (unlikely(idx >= MAX_NODES)) {
		lockevent_inc(lock_no_node);
		while (!queued_spin_trylock(lock))
			cpu_relax();
		goto release;
	}

	// 获取队列结点
	node = grab_mcs_node(node, idx);

	/*
	 * Keep counts of non-zero index values:
	 */
	lockevent_cond_inc(lock_use_node2 + idx - 1, idx);

	/*
	 * Ensure that we increment the head node->count before initialising
	 * the actual node. If the compiler is kind enough to reorder these
	 * stores, then an IRQ could overwrite our assignments.
	 */
	 // 中断到来前要保证node0->count要与中断处理函数需要使用的node对应
	barrier();

	node->locked = 0;
	node->next = NULL;
	pv_init_node(node);

	/*
	 * We touched a (possibly) cold cacheline in the per-cpu queue node;
	 * attempt the trylock once more in the hope someone let go while we
	 * weren't watching.
	 */
	if (queued_spin_trylock(lock))
		goto release;

	/*
	 * Ensure that the initialisation of @node is complete before we
	 * publish the updated tail via xchg_tail() and potentially link
	 * @node into the waitqueue via WRITE_ONCE(prev->next, node) below.
	 */
	 // 写内存屏障，用于保证不同node的初始化/加入队列操作能按顺序执行
	smp_wmb();

	/*
	 * Publish the updated tail.
	 * We have already touched the queueing cacheline; don't bother with
	 * pending stuff.
	 *
	 * p,*,* -> n,*,*
	 */
	old = xchg_tail(lock, tail);
	next = NULL;

	/*
	 * if there was a previous node; link it and wait until reaching the
	 * head of the waitqueue.
	 */
	 // 之前队列非空
	if (old & _Q_TAIL_MASK) {
		prev = decode_tail(old);

		/* Link @node into the waitqueue. */
		// 将当前结点加入等待队列，及链接到上一个结点的next
		WRITE_ONCE(prev->next, node);

		pv_wait_node(node, prev);
		// 自旋等待当前结点的locked域变为1(上一个结点实现)
		// 队列的头节点自旋等待锁的locked域和pending域都变成0，队列中后面的节点自旋等待节点内的locked域变成1
		arch_mcs_spin_lock_contended(&node->locked);

		/*
		 * While waiting for the MCS lock, the next pointer may have
		 * been set by another lock waiter. We optimistically load
		 * the next pointer & prefetch the cacheline for writing
		 * to reduce latency in the upcoming MCS unlock operation.
		 */
		next = READ_ONCE(node->next);
		if (next)
			prefetchw(next); // 从内存中预读数据到cache 中
	}

	/*
	 * we're at the head of the waitqueue, wait for the owner & pending to
	 * go away.
	 *
	 * *,x,y -> *,0,0
	 *
	 * this wait loop must use a load-acquire such that we match the
	 * store-release that clears the locked bit and create lock
	 * sequentiality; this is because the set_locked() function below
	 * does not imply a full barrier.
	 *
	 * The PV pv_wait_head_or_lock function, if active, will acquire
	 * the lock and return a non-zero value. So we have to skip the
	 * atomic_cond_read_acquire() call. As the next PV queue head hasn't
	 * been designated yet, there is no way for the locked value to become
	 * _Q_SLOW_VAL. So both the set_locked() and the
	 * atomic_cmpxchg_relaxed() calls will be safe.
	 *
	 * If PV isn't active, 0 will be returned instead.
	 *
	 */
	 // 队列头结点，在出队列去抢锁时会将后一个结点的locked域置为1，后一个结点将变成队列头结点
	 // 若走到这里，说明当前结点的locked域已被置1，当前结点已成为队列头结点
	if ((val = pv_wait_head_or_lock(lock, node)))
		goto locked;

	// 等待锁的locked域和pending域都变成0
	val = atomic_cond_read_acquire(&lock->val, !(VAL & _Q_LOCKED_PENDING_MASK));

locked:
	/*
	 * claim the lock:
	 *
	 * n,0,0 -> 0,0,1 : lock, uncontended
	 * *,*,0 -> *,*,1 : lock, contended
	 *
	 * If the queue head is the only one in the queue (lock value == tail)
	 * and nobody is pending, clear the tail code and grab the lock.
	 * Otherwise, we only need to grab the lock.
	 */

	/*
	 * In the PV case we might already have _Q_LOCKED_VAL set, because
	 * of lock stealing; therefore we must also allow:
	 *
	 * n,0,1 -> 0,0,1
	 *
	 * Note: at this point: (val & _Q_PENDING_MASK) == 0, because of the
	 *       above wait condition, therefore any concurrent setting of
	 *       PENDING will make the uncontended transition fail.
	 */
	 // 当前结点为队列中最后一个(唯一一个)结点
	if ((val & _Q_TAIL_MASK) == tail) {
		// 若发生竞争，val被改
		// 1. 有新的持锁请求，未走队列模式，误占了pending，需等新的请求清pending走队列模式
		// 2. 有新的持锁请求，走队列模式，修改了tail
		// 若没有竞争持锁并退出
		if (atomic_try_cmpxchg_relaxed(&lock->val, &val, _Q_LOCKED_VAL))
			goto release; /* No contention */
	}

	/*
	 * Either somebody is queued behind us or _Q_PENDING_VAL got set
	 * which will then detect the remaining tail and queue behind us
	 * ensuring we'll see a @next.
	 */
	set_locked(lock);

	/*
	 * contended path; wait for next if not observed yet, release.
	 */
	 // 获取下一个结点
	if (!next)
		next = smp_cond_load_relaxed(&node->next, (VAL));

	// 置下一个结点的locked域，下一个结点变成头结点
	arch_mcs_spin_unlock_contended(&next->locked);
	pv_kick_node(lock, next);

release:
	trace_contention_end(lock, 0);

	/*
	 * release the node
	 */
	 // 释放node，减少引用计数
	__this_cpu_dec(qnodes[0].mcs.count);
}
EXPORT_SYMBOL(queued_spin_lock_slowpath);
```



pvspinlock

numa_aware





# 三、参考链接

https://mp.weixin.qq.com/s?__biz=Mzg2OTc0ODAzMw==&mid=2247506798&idx=1&sn=85e17172c358eff564fa7c32fc8605b8&chksm=ce9ac740f9ed4e568d024853f76b4c72fe0fff80b2a4d1ec25f58bc7941a9ddf789a90e757ab&mpshare=1&scene=1&srcid=0808ZpQzrA0wK1RmrfrmhghG&sharer_sharetime=1670559876045&sharer_shareid=1030c4de44176171b7eb4b7240ae2aaa&key=9e9d434f324e205c11953ffec037e07d218ad1a5ebb4432affb7508f10bd48d014fde0f435df93d1c3f179425f041aa66ebdfdb69d745a4af7e860f4e6ce433882d71e8d817ba6d399ce58379a42ff785ef1223fa0cb95061f79f9a132e3b85345d7e03d971db69124023e66f45a8d5089f4a0d9a0421f26d50a10d684454d63&ascene=1&uin=MTM2NzY2MzQ4MA%3D%3D&devicetype=Windows+10+x64&version=63080029&lang=zh_CN&exportkey=n_ChQIAhIQeV7A8YIBDHLXZqloQh6lUhLqAQIE97dBBAEAAAAAAG3KB7OIVhQAAAAOpnltbLcz9gKNyK89dVj0zpyfji9cxbWCco2TPrZpkM24nL84CgHea16Tc5I574Khv%2Bi%2F1keYzKRLbrQyanovpmz1tV0Ddlsnth8%2B2sdgqTPyIgBMdqbq1wh%2FGPN5bm2%2FAXPULFl4%2Fjk302FheOtCY%2FNOrKKwNz4H4WriFOsxVW3Q5PmNGHjInsVWQXraKdJkLtYDnUTenYcIYgIeVr1VMlUT10OGqUqKi7t2xFEPtquQes8eTcsM%2Bj%2BfZZaLb7RS0hjtdFRG6aABCU5%2FU6kd18%2BQ5A%3D%3D&acctmode=0&pass_ticket=o%2Fz9SwGKko9VSJeV3FTzm4ShxILPsx5nOjTRmBQrKu9ey1XS0pt%2FAKidyeawUfWfGLf19sEpign1NnaA9ZpiUQ%3D%3D&wx_header=1&fontgear=3



https://blog.csdn.net/Roland_Sun/article/details/107086021
