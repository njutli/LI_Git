
# 1、介绍一下ceph的相关角色，包括但不限于mon,mgr,osd,mds,rgw,pg

# 2、以linux内核存储栈类比，我理解MON管理的状态就像raid的拓扑，是相对稳定且关键的基本状态，MGR管理的状态就像iostat工具查询的反应系统即时IO状态的一些临时状态

# 3、ceph和cephfs是什么关系，这两者是必须同时存在，还是可以单独存在，我理解ceph是管理整个存储集群的，cephfs是单个存储节点上面上用户进行数据交互的

# 我找到一张ceph逻辑结构图的图，同时我看了下内核中cephfs的读写接口，内核中用户的读写请求最终是通过网络发送出去，我理解这个网络请求的终点就是ceph，而cephfs是和图中S3/SWIFT同级的概念，一端为用户提供符合posix语义的接口，另一端与ceph集群交互。总的来说，cephfs/S3/SWIFT这一级，相当于在用户一侧给不同的用户应用场景提供不同的接口，与此同时，ceph一侧，也有对应的服务匹配这些不同的接口


# ceph集群是不是由部署着MON和OSD的多个结点组成，结合你描述的内核里得的cephfs路径，cephfs先向部署MON的结点发送请求获取元数据，再根据元数据确定数据发送给哪个部署着OSD的结点？


# PG和OSD是怎样的对应关系，是多对多吗，我理解一个PG对应多个OSD，意味着一个文件切片的多个副本存储在多个位置上，一个OSD对应多个PG，意味着一个存储设备上可以村多个不同的文件切片，是这样吗


# 为什么要有PG和OSD这样的概念，假如没有集群，只是单机存储，我理解通过raid的方式也能实现多副本，PG和OSD是在多主机集群的层级实现类似raid的多副本功能吗

# 介绍一下ceph从用户读写文件操作到数据落入实际物理盘的存储栈，包括这个过程中数据的转换处理过程，例如文件数据可能涉及到的切片，如何转换成对象，如何实现多副本等

# 同一个ceph集群，可以同时支持多种存储场景吗，例如cephfs和S3/SWITCH，对于cephfs而已，作为一个文件系统，肯定有文件系统对应的元数据，而S3/SWITCH是不是不一样，两种场景要存储的数据类型和数据转换过程能同时支持吗
