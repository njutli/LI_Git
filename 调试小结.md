# 1、ftrace

Ftrace有两大组成部分，framework和一系列的tracer 。每个tracer完成不同的功能，它们统一由framework管理。 ftrace 的trace信息保存在ring buffer中，由framework负责管理。Framework 利用debugfs建立tracing目录，并提供了一系列的控制文件

挂载调试目录

```
mount -t debugfs none /sys/kernel/debug
或者指定到自己的目录
mkdir /debug
mount -t debugfs nodev /debug
```

以Function tracer为例，结果存储在 **trace**，该文件类似一张报表，该表将显示 4 列信息。首先是进程信息，包括进程名和PID ；第二列是CPU；第三列是时间戳；第四列是函数信息，缺省情况下，这里将显示内核函数名以及它的上一层调用函数。

```
cd /sys/kernel/debug/tracing
echo function > current_tracer
cat trace

# tracer: function
#
#  TASK-PID   CPU#  TIMESTAMP        FUNCTION
#   |  |       |  
gmond-6684  [004] 13285965.088308: _spin_lock <-hrtimer_interrupt
gmond-6684  [004] 13285965.088308: ktime_get_update_offsets <-hrtimer_interrupt
gmond-6684  [004] 13285965.088309: __run_hrtimer <-hrtimer_interrupt
gmond-6684  [004] 13285965.088309: __remove_hrtimer <-__run_hrtimer
gmond-6684  [004] 13285965.088309: tick_sched_timer <-__run_hrtimer
gmond-6684  [004] 13285965.088309: ktime_get <-tick_sched_timer
gmond-6684  [004] 13285965.088310: tick_do_update_jiffies64 <-tick_sched_timer
gmond-6684  [004] 13285965.088310: update_process_times <-tick_sched_timer
```

```text
аvailable_filter_functions  options            stack_trace_filter
available_tracers           per_cpu             trace
buffer_size_kb              printk_formats      trace_clock
buffer_total_size_kb        README              trace_marker
current_tracer              saved_cmdlines      trace_options
dyn_ftrace_total_info       set_event           trace_pipe
enabled_functions           set_ftrace_filter   trace_stat
events                      set_ftrace_notrace  tracing_cpumask
free_buffer                 set_ftrace_pid      tracing_max_latency
function_profile_enabled    set_graph_function  tracing_on
instances                   set_graph_notrace   tracing_thresh
kprobe_events               snapshot            uprobe_events
kprobe_profile              stack_max_size      uprobe_profile
```

- available_tracers —— 可用的跟踪程序
- current_tracer —— 正在运行的跟踪程序
- tracing_on —— 负责启用或禁用数据写入到 Ring 缓冲区的系统文件（如果启用它，数字 1 被添加到文件中，禁用它，数字 0 被添加）
- trace —— 以人类友好格式保存跟踪数据的文件
- set_ftrace_filter —— 过滤出指定的函数
- set_ftrace_notrace —— 过滤掉指定的函数

```
#cat available_tracers
blk kmemtrace function_graph wakeup_rt wakeup function sysprof sched_switch initcall nop
```

- function —— 一个无需参数的函数调用跟踪程序
- function_graph —— 一个使用子调用的函数调用跟踪程序
- blk —— 一个与块 I/O 跟踪相关的调用和事件跟踪程序（它是 blktrace 使用的）
- mmiotrace —— 一个内存映射 I/O 操作跟踪程序
- nop —— 最简单的跟踪程序，就像它的名字所暗示的那样，它不做任何事情
- sched_switch —— 记录系统中的进程切换信息
- irqsoff —— 记录系统在哪里关中断的时间最长
- preemptoff/preemptirqsoff —— 记录禁止抢占时间最长的函数
- wakeup —— 记录系统在哪里调度延迟最长
- sysprof —— 定时对内核采样
- stack_trace —— 记录内核函数的堆栈使用
- branch —— 记录likely/unlikely语句统计结果

使用传统的 ftrace 需要如下几个步骤：

- 选择一种 tracer
- 使能 ftrace
- 执行需要 trace 的应用程序，比如需要跟踪 ls，就执行 ls
- 关闭 ftrace
- 查看 trace 文件

```text
#!/bin/sh

dir=/sys/kernel/debug/tracing

sysctl kernel.ftrace_enabled=1
echo function_graph > ${dir}/current_tracer
echo 1 > ${dir}/tracing_on
sleep 1
echo 0 > ${dir}/tracing_on
less ${dir}/trace
```

/sys/kernel/debug/tracing/ -- trace文件目录

trace_printk -- 将打印打到trace文件中

cat trace_pipe -- 查看trace文件变化

```
/sys/kernel/debug/tracing

向上↑的调用者
echo 0 > tracing_on
echo function > current_tracer
echo 1 > options/func_stack_trace		//function时打栈
echo xxx > set_ftrace_filter			//过滤器
echo 1 > tracing_on
cat trace

向下↓的要调用的函数
echo 0 > tracing_on
echo > set_ftrace_filter				//清空，不然可能会没栈
echo function_graph > current_tracer
echo xxx > set_graph_function			//过滤器
echo 1 > tracing_on
cat trace

cat trace_pipe								//实时管道

```



https://zhuanlan.zhihu.com/p/27190018

https://zhuanlan.zhihu.com/p/39788032

https://www.cnblogs.com/arnoldlu/p/7211249.html

https://www.cnblogs.com/danxi/p/6417828.html

# 2、strace

跟踪系统调用和信号

```
strace  [  -dffhiqrtttTvxx  ] [ -acolumn ] [ -eexpr ] ...
    [ -ofile ] [-ppid ] ...  [ -sstrsize ] [ -uusername ]
    [ -Evar=val ] ...  [ -Evar  ]...
     [command [ arg ...  ] ]

strace  -c  [ -eexpr ] ...  [ -Ooverhead ] [ -Ssortby ]
    [ command [ arg...  ] ]
```

```c
-c 统计每一系统调用的所执行的时间,次数和出错的次数等.
-d 输出strace关于标准错误的调试信息.
-f 跟踪由fork调用所产生的子进程.
-ff 如果提供-o filename,则所有进程的跟踪结果输出到相应的filename.pid中,pid是各进程的进程号.
-F 尝试跟踪vfork调用.在-f时,vfork不被跟踪.
-h 输出简要的帮助信息.
-i 输出系统调用的入口指针.
-k // 打印栈
-q 禁止输出关于脱离的消息.
-r 打印出相对时间关于,,每一个系统调用.
-t 在输出中的每一行前加上时间信息.
-tt 在输出中的每一行前加上时间信息,微秒级.
-ttt 微秒级输出,以秒了表示时间.
-T 显示每一调用所耗的时间.
-v 输出所有的系统调用.一些调用关于环境变量,状态,输入输出等调用由于使用频繁,默认不输出. -- // 打印所有参数
-V 输出strace的版本信息.
-x 以十六进制形式输出非标准字符串
-xx 所有字符串以十六进制形式输出.
-a column 设置返回值的输出位置.默认 为40.
-e expr 指定一个表达式,用来控制如何跟踪.格式：[qualifier=][!]value1[,value2]...
qualifier只能是 trace,abbrev,verbose,raw,signal,read,write其中之一.value是用来限定的符号或数字.默认的 qualifier是 trace.感叹号是否定符号.例如:-eopen等价于 -e trace=open,表示只跟踪open调用.而-etrace!=open 表示跟踪除了open以外的其他调用.有两个特殊的符号 all 和 none. 注意有些shell使用!来执行历史记录里的命令,所以要使用\\.
-e trace=set 只跟踪指定的系统 调用.例如:-e trace=open,close,rean,write表示只跟踪这四个系统调用.默认的为set=all.
-e trace=file 只跟踪有关文件操作的系统调用.
-e trace=process 只跟踪有关进程控制的系统调用.
-e trace=network 跟踪与网络有关的所有系统调用.
-e strace=signal 跟踪所有与系统信号有关的 系统调用
-e trace=ipc 跟踪所有与进程通讯有关的系统调用
-e abbrev=set 设定strace输出的系统调用的结果集.-v 等与 abbrev=none.默认为abbrev=all.
-e raw=set 将指定的系统调用的参数以十六进制显示.
-e signal=set 指定跟踪的系统信号.默认为all.如 signal=!SIGIO(或者signal=!io),表示不跟踪SIGIO信号.
-e read=set 输出从指定文件中读出 的数据.例如: -e read=3,5
-e write=set 输出写入到指定文件中的数据.
-o filename 将strace的输出写入文件filename
-p pid 跟踪指定的进程pid.
-s strsize 指定输出的字符串的最大长度.默认为32.文件名一直全部输出. // 指定足够的长度
-u username 以username的UID和GID执行被跟踪的命令
```

https://zhuanlan.zhihu.com/p/90612811



# 3、GDB

https://blog.csdn.net/chen1415886044/article/details/105094688

https://bbs.huaweicloud.com/blogs/308343

设置参数 set args

查看变量 p

| 命令                      | 简写                | 含义                                        |
| ------------------------- | ------------------- | ------------------------------------------- |
| file <file>               | -                   | 装入待调试的可执行文件                      |
| **run**                   | r                   | 执行程序(至结束)                            |
| **start**                 | -                   | 开始调试(至main开始处暂停)                  |
| **step**                  | s                   | 执行一条程序，若为函数则进入内部执行        |
| **next**                  | n                   | 执行一条程序，不进入函数内部                |
| continue                  | c                   | 连续运行                                    |
| finish                    | -                   | 运行到当前函数返回                          |
| kill                      | k                   | 终止正在调试的程序                          |
| **list**                  | l                   | 列出源代码的一部分(10行)                    |
| **print** <tmp>           | p <tmp>             | 打印变量的值                                |
| **info locals**           | i locals            | 查看当前栈帧的局部变量                      |
| **backtrace**             | bt                  | 查看函数调用栈帧编号                        |
| **frame** <id>            | f <id>              | 选择栈帧(再看局部变量)                      |
| **display** <tmp>         | -                   | 每次自动显示跟踪的变量的值                  |
| undisplay <tmp>           | -                   | 取消跟踪                                    |
| **break** <num>           | b                   | 设置(调试)断点                              |
| delete breakpoints <num>  | d breakpoints <num> | 删除断点，不加行号则删除所有                |
| disable breakpoints <num> | -                   | 屏蔽断点                                    |
| enable breakpoints <num>  | -                   | 启用断点                                    |
| **info breakpoints**      | i breakpoints       | 显示所有断点                                |
| break 9 if sum != 0       | -                   | 根据条件设置断点(sum不等于0时，第9行设断点) |
| **set var** sum=0         | -                   | 修改变量的值(使sum变量的值为0)              |
| watch <tmp>               | -                   | 监视一个变量的值                            |
| examine <...>             | -                   | 查看内存中的地址                            |
| jump <num>                | j                   | 跳转执行                                    |
| signal <...>              | -                   | 产生信号量                                  |
| return                    | -                   | 强制函数返回                                |
| call <fun>                | -                   | 强制调用函数                                |
| make <...>                | -                   | 不退出gdb下重新产生可执行文件               |
| shell <...>               | -                   | 不退出gdb下执行shell命令                    |
| **quit**                  | q                   | 退出gdb环境                                 |

设置断点：

函数参数为特定值断住
b func if a == 10

特定文件行数断住
b file.c:6

查看调用栈
bt

查看内存
x /20xh 0x7fffffffe080

设置变量
set var offset=4084

打印结构体
set p pretty on
p *struct

# 4、vmlinux

## crash调试

https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/deployment_guide/s1-kdump-crash

https://os.51cto.com/article/648410.html

系统崩溃重启后在对应目录取vmcore与vmlinux(如果是.bz压缩文件，需要先解压)

使用crash工具，通过vmlinux解析vmcore

**加载模块：mod -s  (对于=m编译的模块，需用crash加载后才能解析vmcore)**

```
[root@R11-iscsi1-bak-x86 vmcore_llf]# ls -lh
total 764M
-r--------. 1 root root  66M Jun 17 02:04 vmcore
-rwx------. 1 root root 698M Jun 17 02:04 vmlinux-5.10.0-60.18.0.h217.eulerosv2r11.x86_64+
[root@R11-iscsi1-bak-x86 vmcore_llf]# cd ..
[root@R11-iscsi1-bak-x86 vmcore-ext4]# ls
bt-foreach.txt  crash  ext4.ko  iscsi2-ext4  jbd2.ko  kernel-debuginfo-5.10.0-60.18.0.h152.kasan.eulerosv2r11.x86_64.rpm  log  vmcore_llf  vmlinux
[root@R11-iscsi1-bak-x86 vmcore-ext4]# ./crash vmcore_llf/vmcore vmcore_llf/vmlinux-5.10.0-60.18.0.h217.eulerosv2r11.x86_64+

crash 7.3.0-5.eulerosv2r11
Copyright (C) 2002-2021  Red Hat, Inc.
Copyright (C) 2004, 2005, 2006, 2010  IBM Corporation
Copyright (C) 1999-2006  Hewlett-Packard Co
Copyright (C) 2005, 2006, 2011, 2012  Fujitsu Limited
Copyright (C) 2006, 2007  VA Linux Systems Japan K.K.
Copyright (C) 2005, 2011, 2020-2021  NEC Corporation
Copyright (C) 1999, 2002, 2007  Silicon Graphics, Inc.
Copyright (C) 1999, 2000, 2001, 2002  Mission Critical Linux, Inc.
This program is free software, covered by the GNU General Public License,
and you are welcome to change it and/or distribute copies of it under
certain conditions.  Enter "help copying" to see the conditions.
This program has absolutely no warranty.  Enter "help warranty" for details.

GNU gdb (GDB) 7.6
Copyright (C) 2013 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type "show copying"
and "show warranty" for details.
This GDB was configured as "x86_64-unknown-linux-gnu"...

WARNING: kernel relocated [574MB]: patching 121628 gdb minimal_symbol values

      KERNEL: vmcore_llf/vmlinux-5.10.0-60.18.0.h217.eulerosv2r11.x86_64+
    DUMPFILE: vmcore_llf/vmcore  [PARTIAL DUMP]
        CPUS: 4
        DATE: Tue Jun  7 17:43:00 CST 2022
      UPTIME: 06:47:07
LOAD AVERAGE: 1.43, 1.50, 1.41
       TASKS: 207
    NODENAME: client2
     RELEASE: 5.10.0-60.18.0.h217.eulerosv2r11.x86_64+
     VERSION: #1 SMP Tue Jun 7 02:35:59 UTC 2022
     MACHINE: x86_64  (3000 Mhz)
      MEMORY: 4 GB
       PANIC: "Kernel panic - not syncing: sysrq triggered crash"
         PID: 719605
     COMMAND: "bash"
        TASK: ffff9b7c548ac180  [THREAD_INFO: ffff9b7c548ac180]
         CPU: 2
       STATE: TASK_RUNNING (PANIC)

crash> bt //使用bt 命令用于查看系统崩溃前的堆栈信息
PID: 719605  TASK: ffff9b7c548ac180  CPU: 2   COMMAND: "bash"
 #0 [ffffbac08803bde8] panic at ffffffffa580c375
 #1 [ffffbac08803be68] sysrq_handle_crash at ffffffffa5484516
 #2 [ffffbac08803be70] __handle_sysrq.cold at ffffffffa5834b78
 #3 [ffffbac08803bea0] write_sysrq_trigger at ffffffffa5484de4
 #4 [ffffbac08803beb8] proc_reg_write at ffffffffa51fca00
 #5 [ffffbac08803bed0] vfs_write at ffffffffa5159fde
 #6 [ffffbac08803bf08] ksys_write at ffffffffa515c45f
 #7 [ffffbac08803bf40] do_syscall_64 at ffffffffa58538e3
 #8 [ffffbac08803bf50] entry_SYSCALL_64_after_hwframe at ffffffffa5a0007c
    RIP: 00007fac9bf5bf17  RSP: 00007fffc65540c8  RFLAGS: 00000246
    RAX: ffffffffffffffda  RBX: 0000000000000002  RCX: 00007fac9bf5bf17
    RDX: 0000000000000002  RSI: 000055e7ed71ce00  RDI: 0000000000000001
    RBP: 000055e7ed71ce00   R8: 00007fac9c010000   R9: 00007fac9c010080
    R10: 00007fac9c00ff80  R11: 0000000000000246  R12: 0000000000000002
    R13: 00007fac9c0515a0  R14: 0000000000000002  R15: 00007fac9c0517a0
    ORIG_RAX: 0000000000000001  CS: 0033  SS: 002b
crash>


```



![image-20220607194427765](C:\Users\l00503603\AppData\Roaming\Typora\typora-user-images\image-20220607194427765.png)

根据堆栈信息反汇编，确认源码位置

查看CPU运行的进程
last -C 48

查看对应版本ko
ls /lib/modules/

查看文件系统超级块
mount

查看链表中所有节点

[2022-11-28 09:43:28]  ffff8abce43fc500 ffff8abc00a6f000 tmpfs  tmpfs     /run

struct super_block {
[1352] struct list_head s_inodes;
}

ffff8abc00a6f000 + 1352 = 0xffff8abc00a6f548

list inode.i_sb_list -s inode.i_data.nrpages -H 0xffff8abc00a6f548

查看进程栈
bt pid

读percpu变量
struct call_single_data_t 0x4b900:all |grep :

查看list_head

```
[2023-09-04 20:17:04]      list = {
[2023-09-04 20:17:04]        next = 0xffff888118b824a8, 
[2023-09-04 20:17:04]        prev = 0xffff888112b0a488
[2023-09-04 20:17:04]      }, 
[2023-09-04 20:17:08]  crash> list 0xffff888118b824a8
```

查看汇编
dis xfs_wait_buftarg

查看函数地址

```
crash> p xfs_wait_buftarg
xfs_wait_buftarg = $8 =
 {void (struct xfs_buftarg *)} 0xffffffffc14a94d0 <xfs_wait_buftarg>
crash>
```

查看所有进程栈中的指定内容

```
crash> foreach bt | grep xfs_wait_buftarg
 #3 [ffff888103c97a98] xfs_wait_buftarg at ffffffffc14a957a [xfs]
crash>
```

查看历史的栈

```
bt -r
```

根据bio_set查看bio

```
crash> p fs_bio_set
fs_bio_set = $9 = {
  bio_slab = 0xffff888100d9c000,
  front_pad = 0,
  bio_pool = {
...
crash> struct kmem_cache 0xffff888100d9c000
struct kmem_cache {
  cpu_slab = 0x4c8e0,
  flags = 1207967744,
  min_partial = 5,
  size = 320,
  object_size = 216,
  reciprocal_size = {
    m = 2576980378,
    sh1 = 1 '\001',
    sh2 = 8 '\b'
  },
  offset = 104,
  cpu_partial = 13,
  oo = {
    x = 65561
  },
  max = {
    x = 65561
  },
  min = {
    x = 12
  },
  allocflags = 262144,
  refcount = 1,
  ctor = 0x0,
  inuse = 216,
  align = 64,
  red_left_pad = 0,
  name = 0xffff888100d7a398 "bio-0",
  list = {
    next = 0xffff888100d9ce68,
    prev = 0xffff888100d9d868
  },
...
crash> kmem -S bio-0
CACHE             OBJSIZE  ALLOCATED     TOTAL  SLABS  SSIZE  NAME
ffff888100d9c000      216         43      1275     51     8k  bio-0
CPU 0 KMEM_CACHE_CPU:
  ffff88811a44c8e0
CPU 0 SLAB:
  SLAB              MEMORY            NODE  TOTAL  ALLOCATED  FREE
  ffffea0004167680  ffff8881059da000     0     25          2    23
  FREE / [ALLOCATED]
   ffff8881059da000  (cpu 0 cache)
   ffff8881059da140  (cpu 0 cache)
   ffff8881059da280  (cpu 0 cache)
   ffff8881059da3c0  (cpu 0 cache)
   ffff8881059da500  (cpu 0 cache)
   ffff8881059da640  (cpu 0 cache)
   ffff8881059da780  (cpu 0 cache)
   ffff8881059da8c0  (cpu 0 cache)
   ffff8881059daa00  (cpu 0 cache)
   ffff8881059dab40  (cpu 0 cache)
  [ffff8881059dac80]
  [ffff8881059dadc0]
   ffff8881059daf00  (cpu 0 cache)
   ffff8881059db040  (cpu 0 cache)
   ffff8881059db180  (cpu 0 cache)
   ffff8881059db2c0  (cpu 0 cache)

```

### 案例3 - 进程下发IO触发hungtask

```
crash_8.0_arm64> ps | grep dio
  1764442      2  75  ffff202104298000  ID   0.0       0      0  [dio/dm-2]
  1784075  1784073  49  ffff2020602ce040  IN   0.0    2612   1596  dio_truncate
  1784076  1784075  50  ffff20228ecb6040  UN   0.0    2616   1600  dio_truncate
  1784077  1784076  51  ffff202004688000  UN   0.0    2616   1408  dio_truncate
  1784078  1784076  52  ffff20200468cd00  UN   0.0    2616   1408  dio_truncate
  1784079  1784076  53  ffff202004689340  UN   0.0    2616   1408  dio_truncate
  1784080  1784076  54  ffff202000fde040  UN   0.0    2616   1344  dio_truncate
  1784081  1784076  55  ffff2020225ba680  UN   0.0    2616   1408  dio_truncate
  1784082  1784076  56  ffff2020225bb9c0  UN   0.0    2616   1408  dio_truncate
  1784083  1784076  55  ffff2020225b9340  UN   0.0    2616   1408  dio_truncate
  1784084  1784076  58  ffff2020225b8000  UN   0.0    2616   1408  dio_truncate
  1784085  1784076  63  ffff202002bb4d00  UN   0.0    2616   1408  dio_truncate
  1784086  1784076  60  ffff202002bb0000  UN   0.0    2616   1344  dio_truncate
  1784087  1784076  61  ffff202002bb39c0  UN   0.0    2616   1408  dio_truncate
  1784088  1784076  62  ffff202002bb2680  UN   0.0    2616   1408  dio_truncate
  1784089  1784076  69  ffff202002bb1340  UN   0.0    2616   1408  dio_truncate
  1784090  1784076  64  ffff202012884d00  UN   0.0    2616   1408  dio_truncate
  1784091  1784076   2  ffff202012881340  UN   0.0    2616   1408  dio_truncate
  1784092  1784076  67  ffff202012886040  UN   0.0    2616   1408  dio_truncate
crash_8.0_arm64>
crash_8.0_arm64> bt 1784076
PID: 1784076  TASK: ffff20228ecb6040  CPU: 50  COMMAND: "dio_truncate"
 #0 [ffff80002b2a3b00] __switch_to at ffff8000100198dc
 #1 [ffff80002b2a3b20] __schedule at ffff800010cf7490
 #2 [ffff80002b2a3bb0] schedule at ffff800010cf790c
 #3 [ffff80002b2a3bd0] rwsem_down_write_slowpath at ffff80001012fdbc
 #4 [ffff80002b2a3c90] down_write at ffff800010cfb84c
 #5 [ffff80002b2a3ca0] ext4_dio_write_iter at ffff800009106d7c [ext4]
 #6 [ffff80002b2a3d10] ext4_file_write_iter at ffff8000091070ac [ext4]
 #7 [ffff80002b2a3d20] new_sync_write at ffff8000103f317c
 #8 [ffff80002b2a3db0] vfs_write at ffff8000103f5ea4
 #9 [ffff80002b2a3df0] ksys_write at ffff8000103f61b0
#10 [ffff80002b2a3e30] __arm64_sys_write at ffff8000103f6260
#11 [ffff80002b2a3e40] el0_svc_common.constprop.0 at ffff80001002bd6c
#12 [ffff80002b2a3e70] do_el0_svc at ffff80001002bed8
#13 [ffff80002b2a3e80] el0_svc at ffff800010cf0e5c
#14 [ffff80002b2a3ea0] el0_sync_handler at ffff800010cf175c
#15 [ffff80002b2a3fe0] el0_sync at ffff80001001255c
     PC: 0000e7ffe2ddc490   LR: 0000000000406608   SP: 0000ffffddcec240
    X29: 0000ffffddcec240  X28: 0000000000000064  X27: 0000000000010000
    X26: 0000000000000001  X25: 0000000000000000  X24: 0000000000000030
    X23: 000000000041ecd8  X22: 0000e7ffe2efb7e0  X21: 0000000000000003
    X20: 0000000000010000  X19: 0000000000000003  X18: 0000000000000000
    X17: 0000e7ffe2ddc460  X16: 0000000000440340  X15: 0000000000000088
    X14: 0000000000000000  X13: 0000000000000000  X12: 0000ffffddcebcd0
    X11: 0000000000000000  X10: 0000e7ffe2d0d608   X9: 0000e7ffe2f14b40
     X8: 0000000000000040   X7: 000000000152b000   X6: 0000000000010000
     X5: 000000000152b000   X4: 0000000000000003   X3: 0000e7ffe2efb020
     X2: 0000000000010000   X1: 000000000152b000   X0: 0000000000000003
    ORIG_X0: 0000000000000003  SYSCALLNO: 40  PSTATE: 20001000
crash_8.0_arm64>
crash_8.0_arm64> bt 1784076 -f
PID: 1784076  TASK: ffff20228ecb6040  CPU: 50  COMMAND: "dio_truncate"
 #0 [ffff80002b2a3b00] __switch_to at ffff8000100198dc
    ffff80002b2a3b00: ffff80002b2a3b20 ffff800010cf7494
    ffff80002b2a3b10: ffff2037ffd02940 ffff202000220000
 #1 [ffff80002b2a3b20] __schedule at ffff800010cf7490
    ffff80002b2a3b20: ffff80002b2a3bb0 ffff800010cf7910
    ffff80002b2a3b30: ffff20228ecb6040 ffff20228ecb6040
    ffff80002b2a3b40: ffff80002b2a3c58 ffff2021b1809558
    ffff80002b2a3b50: ffff2021b1809554 ffff20228ecb6040
    ffff80002b2a3b60: ffff20228ecb6040 0000000000000002
    ffff80002b2a3b70: ffff80002b2a3c48 ffff2021b1809540
    ffff80002b2a3b80: ffff2021b1809554 ffff800011f10000
    ffff80002b2a3b90: ffff80002b37b968 ffff80002b303968
    ffff80002b2a3ba0: 0000000000000004 c175cbf33848d600
 #2 [ffff80002b2a3bb0] schedule at ffff800010cf790c
    ffff80002b2a3bb0: ffff80002b2a3bd0 ffff80001012fdc0
    ffff80002b2a3bc0: 0000000000000001 0000000000000000
 #3 [ffff80002b2a3bd0] rwsem_down_write_slowpath at ffff80001012fdbc
    ffff80002b2a3bd0: ffff80002b2a3c90 ffff800010cfb850
    ffff80002b2a3be0: ffff80002b2a3d78 0000000000310000
    ffff80002b2a3bf0: ffff80002b2a3d50 ffff2021b18094a0
    ffff80002b2a3c00: ffff2021b1809540 0000000000000000
    ffff80002b2a3c10: 0000000000300000 0000000000000000
    ffff80002b2a3c20: 0000000000000000 ffff20228ecb6040
    ffff80002b2a3c30: ffff8000119e9d60 0000000000000006
    ffff80002b2a3c40: ffff80002b2a3d78 0000000000000001
    ffff80002b2a3c50: ffff80002b2a3c48 ffff80002b2fbc58
    ffff80002b2a3c60: ffff2021b1809558 ffff20228ecb6040
    ffff80002b2a3c70: ffff202100000000 000000010021c493
    ffff80002b2a3c80: ffff80002b2a3ca0 c175cbf33848d600
 #4 [ffff80002b2a3c90] down_write at ffff800010cfb84c
    ffff80002b2a3c90: ffff80002b2a3ca0 ffff800009106d80
 #5 [ffff80002b2a3ca0] ext4_dio_write_iter at ffff800009106d7c [ext4]
    ffff80002b2a3ca0: ffff80002b2a3d10 ffff8000091070b0
    ffff80002b2a3cb0: ffff202103ca8780 ffff80002b2a3e20
    ffff80002b2a3cc0: 000000000152b000 ffffffffffffffea
    ffff80002b2a3cd0: ffff80002b2a3e20 0000000000000000
    ffff80002b2a3ce0: 0000000000000000 0000000000000000
    ffff80002b2a3cf0: 0000000000000000 0000000000000002
    ffff80002b2a3d00: 000080002b2a3db0 c175cbf33848d600
 #6 [ffff80002b2a3d10] ext4_file_write_iter at ffff8000091070ac [ext4]
    ffff80002b2a3d10: ffff80002b2a3d20 ffff8000103f3180
 #7 [ffff80002b2a3d20] new_sync_write at ffff8000103f317c
    ffff80002b2a3d20: ffff80002b2a3db0 ffff8000103f5ea8
    ffff80002b2a3d30: 0000000000010000 ffff202103ca8780
    ffff80002b2a3d40: 000000000152b000 0000000000010000
    ffff80002b2a3d50: ffff800000000005 0000000000000000
    ffff80002b2a3d60: 0000000000010000 ffff80002b2a3d40
    ffff80002b2a3d70: 0000000000000001 ffff202103ca8780
    ffff80002b2a3d80: 0000000000300000 0000000000000000
    ffff80002b2a3d90: 0000000000000000 0000000000020000
    ffff80002b2a3da0: 0000000000000000 c175cbf33848d600
 #8 [ffff80002b2a3db0] vfs_write at ffff8000103f5ea4
    ffff80002b2a3db0: ffff80002b2a3df0 ffff8000103f61b4
    ffff80002b2a3dc0: ffff202103ca8780 ffff202103ca8780
    ffff80002b2a3dd0: 000000000152b000 0000000000010000
    ffff80002b2a3de0: 0000000020000000 00000000000023c0
 #9 [ffff80002b2a3df0] ksys_write at ffff8000103f61b0
    ffff80002b2a3df0: ffff80002b2a3e30 ffff8000103f6264
    ffff80002b2a3e00: ffff80002b2a3eb0 0000000000000040
    ffff80002b2a3e10: 0000000000000200 ffff800010d217a8
    ffff80002b2a3e20: 0000000000300000 c175cbf33848d600
#10 [ffff80002b2a3e30] __arm64_sys_write at ffff8000103f6260
    ffff80002b2a3e30: ffff80002b2a3e40 ffff80001002bd70
#11 [ffff80002b2a3e40] el0_svc_common.constprop.0 at ffff80001002bd6c
    ffff80002b2a3e40: ffff80002b2a3e70 ffff80001002bedc
    ffff80002b2a3e50: ffff80002b2a3eb0 ffffa037ee65d000
    ffff80002b2a3e60: 00000000ffffffff 0000e7ffe2ddc490
#12 [ffff80002b2a3e70] do_el0_svc at ffff80001002bed8
    ffff80002b2a3e70: ffff80002b2a3e80 ffff800010cf0e60
#13 [ffff80002b2a3e80] el0_svc at ffff800010cf0e5c
    ffff80002b2a3e80: ffff80002b2a3ea0 ffff800010cf1760
    ffff80002b2a3e90: 0000000000000200 0000e7ffe2de112c
#14 [ffff80002b2a3ea0] el0_sync_handler at ffff800010cf175c
    ffff80002b2a3ea0: ffff80002b2a3fe0 ffff800010012560
    ffff80002b2a3eb0: 0000000000000003 000000000152b000
    ffff80002b2a3ec0: 0000000000010000 0000e7ffe2efb020
    ffff80002b2a3ed0: 0000000000000003 000000000152b000
    ffff80002b2a3ee0: 0000000000010000 000000000152b000
    ffff80002b2a3ef0: 0000000000000040 0000e7ffe2f14b40
    ffff80002b2a3f00: 0000e7ffe2d0d608 0000000000000000
    ffff80002b2a3f10: 0000ffffddcebcd0 0000000000000000
    ffff80002b2a3f20: 0000000000000000 0000000000000088
    ffff80002b2a3f30: 0000000000440340 0000e7ffe2ddc460
    ffff80002b2a3f40: 0000000000000000 0000000000000003
    ffff80002b2a3f50: 0000000000010000 0000000000000003
    ffff80002b2a3f60: 0000e7ffe2efb7e0 000000000041ecd8
    ffff80002b2a3f70: 0000000000000030 0000000000000000
    ffff80002b2a3f80: 0000000000000001 0000000000010000
    ffff80002b2a3f90: 0000000000000064 0000ffffddcec240
    ffff80002b2a3fa0: 0000000000406608 0000ffffddcec240
    ffff80002b2a3fb0: 0000e7ffe2ddc490 0000000020001000
    ffff80002b2a3fc0: 0000000000000003 0000000000000040
    ffff80002b2a3fd0: 0000000000000000 0000000000000000
#15 [ffff80002b2a3fe0] el0_sync at ffff80001001255c
     PC: 0000e7ffe2ddc490   LR: 0000000000406608   SP: 0000ffffddcec240
    X29: 0000ffffddcec240  X28: 0000000000000064  X27: 0000000000010000
    X26: 0000000000000001  X25: 0000000000000000  X24: 0000000000000030
    X23: 000000000041ecd8  X22: 0000e7ffe2efb7e0  X21: 0000000000000003
    X20: 0000000000010000  X19: 0000000000000003  X18: 0000000000000000
    X17: 0000e7ffe2ddc460  X16: 0000000000440340  X15: 0000000000000088
    X14: 0000000000000000  X13: 0000000000000000  X12: 0000ffffddcebcd0
    X11: 0000000000000000  X10: 0000e7ffe2d0d608   X9: 0000e7ffe2f14b40
     X8: 0000000000000040   X7: 000000000152b000   X6: 0000000000010000
     X5: 000000000152b000   X4: 0000000000000003   X3: 0000e7ffe2efb020
     X2: 0000000000010000   X1: 000000000152b000   X0: 0000000000000003
    ORIG_X0: 0000000000000003  SYSCALLNO: 40  PSTATE: 20001000
crash_8.0_arm64>

```



#### （1）确认进程状态

```c
crash_8.0_arm64> ps | grep dio  
  1764442      2  75  ffff202104298000  ID   0.0       0      0  [dio/dm-2]
  1784075  1784073  49  ffff2020602ce040  IN   0.0    2612   1596  dio_truncate
  kernel_wait4
  1784076  1784075  50  ffff20228ecb6040  UN   0.0    2616   1600  dio_truncate
  等写锁
  1784077  1784076  51  ffff202004688000  UN   0.0    2616   1408  dio_truncate
  等读锁
  1784078  1784076  52  ffff20200468cd00  UN   0.0    2616   1408  dio_truncate
  等读锁
  1784079  1784076  53  ffff202004689340  UN   0.0    2616   1408  dio_truncate
  等读锁
  1784080  1784076  54  ffff202000fde040  UN   0.0    2616   1344  dio_truncate
  等读锁
  1784081  1784076  55  ffff2020225ba680  UN   0.0    2616   1408  dio_truncate
  等读锁
  1784082  1784076  56  ffff2020225bb9c0  UN   0.0    2616   1408  dio_truncate
  等读锁
  1784083  1784076  55  ffff2020225b9340  UN   0.0    2616   1408  dio_truncate
  等读操作完成
  1784084  1784076  58  ffff2020225b8000  UN   0.0    2616   1408  dio_truncate
  等读锁
  1784085  1784076  63  ffff202002bb4d00  UN   0.0    2616   1408  dio_truncate
  等读操作完成
  1784086  1784076  60  ffff202002bb0000  UN   0.0    2616   1344  dio_truncate
  等读锁
  1784087  1784076  61  ffff202002bb39c0  UN   0.0    2616   1408  dio_truncate
  等读锁
  1784088  1784076  62  ffff202002bb2680  UN   0.0    2616   1408  dio_truncate
  等读锁
  1784089  1784076  69  ffff202002bb1340  UN   0.0    2616   1408  dio_truncate
  等读操作完成
  1784090  1784076  64  ffff202012884d00  UN   0.0    2616   1408  dio_truncate
  等读锁
  1784091  1784076   2  ffff202012881340  UN   0.0    2616   1408  dio_truncate
  等读操作完成
  1784092  1784076  67  ffff202012886040  UN   0.0    2616   1408  dio_truncate
  等读操作完成
crash_8.0_arm64>
```

**问题模型：**
**读进程占用读锁 --> 写进程阻塞 --> 读进程阻塞**

下一步，确认占用文件锁的进程，以及进程状态

**查找目标struct file的地址**

在函数ksys_write中，通过f.file获取到file的地址，作为参数传递给下一级的vfs_write，因此file的地址应该在vfs_write的栈上可以找到
同时，vfs_write将file的地址作为参数传递给new_sync_write，因此file的地址在new_sync_write栈上也可以找到
观察发现ffff202103ca8780这个地址在这两个函数的栈上都存在，应该是file的地址

```
crash_8.0_arm64> struct file.f_inode ffff202103ca8780
  f_inode = 0xffff2021b18094a0,
crash_8.0_arm64> struct inode.i_rwsem 0xffff2021b18094a0
  i_rwsem = {
    count = {
      counter = 1282
    },
    owner = {
      counter = -246152854740927
    },
    osq = {
      tail = {
        counter = 0
      }
    },
    wait_lock = {
      raw_lock = {
        {
          val = {
            counter = 0
          },
          {
            locked = 0 '\000',
            pending = 0 '\000'
          },
          {
            locked_pending = 0,
            tail = 0
          }
        }
      }
    },
    wait_list = {
      next = 0xffff80002b2a3c58,
      prev = 0xffff80002b333c58
    }
  },

-246152854740927 --> 0xffff202012886041

owner
bit0：是否是读者持有信号，是的话为1
bit1：是否可以在读者持有的信号量上自旋等待
bit2~63：当读者持有信号时，owner中会记录获取到信号的最后一个读者进程的task_struct。如果写者持有信号，那么owner记录的是该写者进程的task_struct

crash_8.0_arm64> kmem 0xffff202012886041
CACHE             OBJSIZE  ALLOCATED     TOTAL  SLABS  SSIZE  NAME
ffff002080007780     4928       1122      2340    390    32k  task_struct
  SLAB              MEMORY            NODE  TOTAL  ALLOCATED  FREE
  fffffe80802a2000  ffff202012880000     2      6          6     0
  FREE / [ALLOCATED]
  [ffff202012886040]

    PID: 1784092
COMMAND: "dio_truncate"
   TASK: ffff202012886040  [THREAD_INFO: ffff202012886040]
    CPU: 67
  STATE: TASK_UNINTERRUPTIBLE

      PAGE         PHYSICAL      MAPPING       INDEX CNT FLAGS
fffffe80802a2180 202012886000 dead000000000400        0  0 57ffff800000000
crash_8.0_arm64>

// 持有信号量进程为1784092

[2023-02-13 17:00:34]  crash_8.0_arm64> bt 1784092
[2023-02-13 17:00:36]  
[2023-02-13 17:00:36]  PID: 1784092  TASK: ffff202012886040  CPU: 67  COMMAND: "dio_truncate"
[2023-02-13 17:00:36]   #0 [ffff80002b37baf0] __switch_to at ffff8000100198dc
[2023-02-13 17:00:36]   #1 [ffff80002b37bb10] __schedule at ffff800010cf7490
[2023-02-13 17:00:36]   #2 [ffff80002b37bba0] schedule at ffff800010cf790c
[2023-02-13 17:00:36]   #3 [ffff80002b37bbc0] io_schedule at ffff800010cf7d24
[2023-02-13 17:00:36]   #4 [ffff80002b37bbe0] blk_io_schedule at ffff8000105d3780
[2023-02-13 17:00:36]   #5 [ffff80002b37bbf0] __iomap_dio_rw at ffff8000104b8338
[2023-02-13 17:00:36]   #6 [ffff80002b37bc90] iomap_dio_rw at ffff8000104b84f8
[2023-02-13 17:00:36]   #7 [ffff80002b37bca0] ext4_dio_read_iter at ffff800009106990 [ext4]
[2023-02-13 17:00:36]   #8 [ffff80002b37bcd0] ext4_file_read_iter at ffff800009106ac8 [ext4]
[2023-02-13 17:00:36]   #9 [ffff80002b37bd10] new_sync_read at ffff8000103f2ff8
[2023-02-13 17:00:36]  #10 [ffff80002b37bda0] vfs_read at ffff8000103f5b60
[2023-02-13 17:00:36]  #11 [ffff80002b37bde0] ksys_read at ffff8000103f6084
[2023-02-13 17:00:36]  #12 [ffff80002b37be30] __arm64_sys_read at ffff8000103f6134
[2023-02-13 17:00:36]  #13 [ffff80002b37be40] el0_svc_common.constprop.0 at ffff80001002bd6c
[2023-02-13 17:00:36]  #14 [ffff80002b37be70] do_el0_svc at ffff80001002bed8
[2023-02-13 17:00:36]  #15 [ffff80002b37be80] el0_svc at ffff800010cf0e5c
[2023-02-13 17:00:36]  #16 [ffff80002b37bea0] el0_sync_handler at ffff800010cf175c
[2023-02-13 17:00:36]  #17 [ffff80002b37bfe0] el0_sync at ffff80001001255c
[2023-02-13 17:00:36]       PC: 0000e7ffe2ddc3c0   LR: 00000000004044a8   SP: 0000ffffddcec2a0
[2023-02-13 17:00:36]      X29: 0000ffffddcec2a0  X28: 0000000000000001  X27: 0000000000423610
[2023-02-13 17:00:36]      X26: 000000000152b000  X25: 0000000000000003  X24: 000000000041ec70
[2023-02-13 17:00:36]      X23: 000000000041eb30  X22: 0000e7ffe2efb7e0  X21: 0000000000060000
[2023-02-13 17:00:36]      X20: 0000000000440c28  X19: 0000000000000003  X18: 0000000000000000
[2023-02-13 17:00:36]      X17: 0000e7ffe2ddc390  X16: 00000000004404f0  X15: 0000e7ffe2e4b048
[2023-02-13 17:00:36]      X14: 00000000001b391c  X13: 0000000000000000  X12: 0000ffffddcebcd0
[2023-02-13 17:00:36]      X11: 0000000000000000  X10: 0000e7ffe2d0d608   X9: 0000e7ffe2f14b40
[2023-02-13 17:00:36]       X8: 000000000000003f   X7: 0000ffffddceb660   X6: 0000000000000000
[2023-02-13 17:00:36]       X5: 0000000000000000   X4: 0000ffffddcebd00   X3: 0000e7ffe2efb020
[2023-02-13 17:00:36]       X2: 0000000000010000   X1: 000000000152b000   X0: 0000000000000003
[2023-02-13 17:00:36]      ORIG_X0: 0000000000000003  SYSCALLNO: 3f  PSTATE: 60000000
[2023-02-13 17:00:36]  
```

持锁进程在等待IO返回

#### （2）确认磁盘IO状态

##### gendisk

查看io状态，需确认对应的gendisk，如果是dm设备，还需确认target设备的gendisk

**操作的文件file --> inode --> superblock --> block_device --> gendisk**

dm设备查找target设备

**gendisk --> mapped_device --> dm_table --> dm_table->devices --> dm_dev_internal --> dm_dev --> block_device(target) --> gendisk(target)**

```
// 查看详细的栈信息
crash_8.0_arm64> bt -FF 1784092
PID: 1784092  TASK: ffff202012886040  CPU: 67  COMMAND: "dio_truncate"
 #0 [ffff80002b37baf0] __switch_to at ffff8000100198dc
    ffff80002b37baf0: ffff80002b37bb10 __schedule+1060
    ffff80002b37bb00: ffff2037fff55940 [ffff202000288000:task_struct]
 #1 [ffff80002b37bb10] __schedule at ffff800010cf7490
    ffff80002b37bb10: ffff80002b37bba0 schedule+80
    ffff80002b37bb20: [ffff202012886040:task_struct] [ffff202012886040:task_struct]
    ffff80002b37bb30: 0000000000000002 ffff80002b37bd68
    ffff80002b37bb40: ext4_iomap_ops   [ffff2021b18094a0:ext4_inode_cache]
    ffff80002b37bb50: 0000000000000010 0000000000000001
    ffff80002b37bb60: 0000000000070000 iomap_dio_actor
    ffff80002b37bb70: ffff80002b37bd40 .mmuoff.data.write
    ffff80002b37bb80: ext4_iomap_ops   [ffff2021b18094a0:ext4_inode_cache]
    ffff80002b37bb90: 0000000000000004 93f2803eddb33000
 #2 [ffff80002b37bba0] schedule at ffff800010cf790c
    ffff80002b37bba0: ffff80002b37bbc0 io_schedule+68
    ffff80002b37bbb0: 0000000000000000 [ffff202012886040:task_struct]
 #3 [ffff80002b37bbc0] io_schedule at ffff800010cf7d24
    ffff80002b37bbc0: ffff80002b37bbe0 blk_io_schedule+68
    ffff80002b37bbd0: [ffff20200622d000:kmalloc-128] __iomap_dio_rw+916
 #4 [ffff80002b37bbe0] blk_io_schedule at ffff8000105d3780
    ffff80002b37bbe0: ffff80002b37bbf0 __iomap_dio_rw+764
 #5 [ffff80002b37bbf0] __iomap_dio_rw at ffff8000104b8338
    ffff80002b37bbf0: ffff80002b37bc90 iomap_dio_rw+24
    ffff80002b37bc00: ffff80002b37bd68 [ffff2021b18094a0:ext4_inode_cache]
    ffff80002b37bc10: ffff80002b37bd40 [ffff2021b1809540:ext4_inode_cache]
    ffff80002b37bc20: ffffffffffffffea ffff80002b37be20
    ffff80002b37bc30: 0000000000000000 0000000000000000
    ffff80002b37bc40: 0000000000000000 [ffff202012886040:task_struct]
    ffff80002b37bc50: [ffff2021b1809618:ext4_inode_cache] 0000000105edc020
    ffff80002b37bc60: ffff80002b37bc60 ffff80002b37bc60
    ffff80002b37bc70: ffff80002b37bc70 ffff80002b37bc70
    ffff80002b37bc80: [ffff202000000000:kmem_cache_node] 93f2803eddb33000
 #6 [ffff80002b37bc90] iomap_dio_rw at ffff8000104b84f8
    ffff80002b37bc90: ffff80002b37bca0 ext4_dio_read_iter+196
 #7 [ffff80002b37bca0] ext4_dio_read_iter at ffff800009106990 [ext4]
    ffff80002b37bca0: ffff80002b37bcd0 ext4_file_read_iter+236
    ffff80002b37bcb0: ffff80002b37bd68 ffff80002b37bd40
    ffff80002b37bcc0: 0000000000000000 [ffff2021b18094a0:ext4_inode_cache]
 #8 [ffff80002b37bcd0] ext4_file_read_iter at ffff800009106ac8 [ext4]
    ffff80002b37bcd0: ffff80002b37bd10 new_sync_read+236
    ffff80002b37bce0: [ffff20200601b5c0:filp] ffff80002b37be20
    ffff80002b37bcf0: 000000000152b000 0000000000000001
    ffff80002b37bd00: ffff80002b37bda0 vfs_read+252
 #9 [ffff80002b37bd10] new_sync_read at ffff8000103f2ff8
    ffff80002b37bd10: ffff80002b37bda0 vfs_read+388
    ffff80002b37bd20: 0000000000010000 [ffff20200601b5c0:filp]
    ffff80002b37bd30: 000000000152b000 0000000000010000
    ffff80002b37bd40: ffff800000000004 0000000000000000
    ffff80002b37bd50: 0000000000000000 ffff80002b37bd40
    ffff80002b37bd60: 0000000000000000 [ffff20200601b5c0:filp]
    ffff80002b37bd70: 0000000000060000 0000000000000000
    ffff80002b37bd80: [ffff202017f90938:request_queue] 0000000000020000
    ffff80002b37bd90: 0000000080070072 93f2803eddb33000
#10 [ffff80002b37bda0] vfs_read at ffff8000103f5b60
    ffff80002b37bda0: ffff80002b37bde0 ksys_read+116
    ffff80002b37bdb0: [ffff20200601b5c0:filp] [ffff20200601b5c0:filp]
    ffff80002b37bdc0: 000000000152b000 0000000000010000

// 下一步查找问题设备
// 根据函数栈中的file信息查找inode
crash_8.0_arm64> file ffff20200601b5c0
struct file {
  f_u = {
    fu_llist = {
      next = 0x0
    },
    fu_rcuhead = {
      next = 0x0,
      func = 0x0
    }
  },
  f_path = {
    mnt = 0xffff204005edc020,
    dentry = 0xffff20405fc8b290
  },
  f_inode = 0xffff2021b18094a0,
  f_op = 0xffff800009177940,

// 根据inode查找super_block
crash_8.0_arm64> inode 0xffff2021b18094a0
struct inode {
  i_mode = 33188,
  i_opflags = 13,
  i_uid = {
    val = 0
  },
  i_gid = {
    val = 0
  },
  i_flags = 5120,
  i_acl = 0x0,
  i_default_acl = 0x0,
  i_op = 0xffff800009177800,
  i_sb = 0xffff204001c86800,
  i_mapping = 0xffff2021b1809618,
  i_security = 0xffff2020f361ec78,

// 根据super_block查找block_device
crash_8.0_arm64> super_block 0xffff204001c86800 | grep s_bdev
  s_bdev = 0xffff202017415780,
crash_8.0_arm64>

// 查看根据block_device查找gendisk
crash_8.0_arm64> block_device 0xffff202017415780 | grep disk
  bd_holder_disks = {
  bd_disk = 0xffff204001fab800,
crash_8.0_arm64>

// 确认设备 --> dm-2
crash_8.0_arm64> gendisk 0xffff204001fab800 | grep disk_name
  disk_name = "dm-2\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000",
crash_8.0_arm64>

// 加载dm_mod
crash_8.0_arm64> mod -s dm_mod dm-mod.ko
     MODULE       NAME                                     BASE           SIZE  OBJECT FILE
ffff800008d84700  dm_mod                             ffff800008d68000   159744  dm-mod.ko
crash_8.0_arm64>

// 通过dm设备的gendisk查找target设备
crash_8.0_arm64> dev -d
MAJOR GENDISK            NAME       REQUEST_QUEUE      TOTAL ASYNC  SYNC
    8 ffff20200745f800   sdc        ffff002094b28000       0     0     0
    8 ffff202009408000   sdb        ffff002094b29270       0     0     0
    8 ffff202006986800   sdd        ffff002099bf0938       6     0     6
    8 ffff20200777c800   sda        ffff002094b28938       0     0     0
    8 ffff202014feb000   sdf        ffff002099bf3750       0     0     0
    8 ffff2020092e6000   sdg        ffff00208e619270       0     0     0
    8 ffff20200949a000   sde        ffff002099bf0000       0     0     0
    8 ffff2020092c7000   sdi        ffff202014ff8000       0     0     0
    8 ffff2020092cb000   sdh        ffff00208e618938       0     0     0
    8 ffff202014f27800   sdk        ffff20201505e568       0     0     0
    8 ffff202006a7e800   sdl        ffff2020150ce568       0     0     0
    8 ffff202000a54800   sdj        ffff20201505dc30       0     0     0
  253 ffff004007739000   dm-0       ffff00401253d2f8       1     0     1
  253 ffff00400a422000   dm-1       ffff00400a5452f8       0     0     0
  253 ffff204001fab800   dm-2       ffff202017f90938       5     5     0
crash_8.0_arm64>

// gendisk->private_data --> mapped_device
crash_8.0_arm64> gendisk ffff204001fab800 | grep private_data
  private_data = 0xffff202002520000,
crash_8.0_arm64>

// mapped_device->map --> dm_table
crash_8.0_arm64> mapped_device 0xffff202002520000 | grep map
struct mapped_device {
  map = 0xffff204004fd4600,
  swap_bios_semaphore = {
crash_8.0_arm64>

// dm_table->devices --> list_head
crash_8.0_arm64> dm_table 0xffff204004fd4600
struct dm_table {
  md = 0xffff202002520000,
  type = DM_TYPE_BIO_BASED,
  depth = 1,
  counts = {1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
  index = {0xffff800012c75000, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0},
  num_targets = 1,
  num_allocated = 8,
  highs = 0xffff800012c75000,
  targets = 0xffff800012c75040,
  immutable_target_type = 0x0,
  integrity_supported = false,
  singleton = false,
  integrity_added = 0,
  mode = 3,
  devices = {
    next = 0xffff202006818080,
    prev = 0xffff202006818080
  },
  event_fn = 0xffff800008d6c940 <event_callback>,
  event_context = 0xffff202002520000,
  mempools = 0x0
}

// dd->list --> dm_dev_internal

// dm_dev_internal->dm_dev --> dm_dev
crash_8.0_arm64> dm_dev_internal 0xffff202006818080
struct dm_dev_internal {
  list = {
    next = 0xffff204004fd46f8,
    prev = 0xffff204004fd46f8
  },
  count = {
    refs = {
      counter = 1
    }
  },
  dm_dev = 0xffff202006818218
}

// dm_dev->bdev --> block_device
crash_8.0_arm64> dm_dev 0xffff202006818218
struct dm_dev {
  bdev = 0xffff0020a1c70a80,
  dax_dev = 0x0,
  mode = 3,
  name = "8:51\000\000\000\000\000\000\000\000\200\373\315\377"
}
crash_8.0_arm64>


// block_device->bd_disk --> gendisk
crash_8.0_arm64> block_device 0xffff0020a1c70a80 | grep disk
  bd_holder_disks = {
  bd_disk = 0xffff202006986800,

// gendisk->disk_name
crash_8.0_arm64> gendisk 0xffff202006986800 | grep name
  disk_name = "sdd\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000d",
        name /sgta\\
      init_name = 0x0,
    name = 0xffff80001107ee70 "integrity",
crash_8.0_arm64>

// target设备为sdd

```



##### request_queue

查看target设备上的IO情况

```
#64个硬件队列批量处理
rd 0xffff8fd390f1f800 64 | awk '{print "blk_mq_hw_ctx " $2}' > blk_mq_hw_ctxs
rd 0xffff8fd390f1f800 64 | awk '{print "blk_mq_hw_ctx " $3}' >> blk_mq_hw_ctxs

#优先sched_tags
crash> < blk_mq_hw_ctxs | grep sched_tags | awk '{print $3}' | awk -F, '{print "blk_mq_tags " $1}' > blk_mq_tags
crash> < blk_mq_tags.txt | grep map | awk '/ffff/ {print "sbitmap_word " $3}' > sbitmap_word.txt
```



**request_queue --> blk_mq_hw_ctx --> blk_mq_tags --> sbitmap_word --> index**

```
// dio ----> dm bio --(clone)--> sdd bio ----> request

// sdd gendisk->queue --> request_queue
crash_8.0_arm64> gendisk ffff202006986800 | grep queue
        wait_queue = {
  queue = 0xffff002099bf0938,
crash_8.0_arm64>

// request_queue->queuedata --> scsi_device
crash_8.0_arm64> request_queue ffff002099bf0938 | grep queuedata
  queuedata = 0xffff00208e391000,
crash_8.0_arm64>

// device_busy有值，说明scsi设备有在执行request
crash_8.0_arm64> scsi_device 0xffff00208e391000
struct scsi_device {
  host = 0xffff20200fa80000,
  request_queue = 0xffff002099bf0938,
  siblings = {
    next = 0xffff00208e390010,
    prev = 0xffff0020993f6010
  },
  same_target_siblings = {
    next = 0xffff00208e439018,
    prev = 0xffff00208e439018
  },
  device_busy = {
    counter = 4
  },

//查找request
request_queue->nr_hw_queues 硬件队列个数
request_queue->queue_hw_ctx 遍历硬件队列

// 当前scsi设备有16个硬件队列
crash_8.0_arm64> request_queue 0xffff002099bf0938 | grep queues
  nr_hw_queues = 16,
crash_8.0_arm64>

// 硬件队列指针数组地址
crash_8.0_arm64> request_queue 0xffff002099bf0938 | grep queue_hw_ctx
  queue_hw_ctx = 0xffff00208f3b2000,
crash_8.0_arm64>

// 第一个硬件队列指针地址
crash_8.0_arm64> rd 0xffff00208f3b2000
ffff00208f3b2000:  ffff00400661f800                    ..a.@...

// 第一个硬件队列
crash_8.0_arm64> blk_mq_hw_ctx ffff00400661f800 | grep queue
  queue = 0xffff002099bf0938,
  queued = 112964,
  queue_num = 0,
crash_8.0_arm64>

// 第一个硬件队列的tags
crash_8.0_arm64> blk_mq_hw_ctx ffff00400661f800 | grep tags
  tags = 0xffff2020110d1e00,
  sched_tags = 0xffff202001718200,
crash_8.0_arm64>

// 有调度器tag，解析调度器tag
crash_8.0_arm64> blk_mq_tags 0xffff202001718200
struct blk_mq_tags {
  nr_tags = 2048,
  nr_reserved_tags = 0,
  active_queues = {
    counter = 0
  },
  pending_queues = {
    counter = 0
  },
  kabi_reserved_bitmap_tags = 0x0,
  kabi_reserved_breserved_tags = 0x0,
  {
    bitmap_tags = {
      sb = {
        depth = 256,
        shift = 6,
        map_nr = 4,
        map = 0xffff20201510a000,
        kabi_reserved1 = 0
      },
      alloc_hint = 0x5da7d1698e9c,
      wake_batch = 8,
      wake_index = {
        counter = 1
      },
      ws = 0xffff20200fa71a00,
      ws_active = {
        counter = 0
      },
      round_robin = true,
      min_shallow_depth = 4294967295
    },

// 第一个word 0x8000000000000000
crash_8.0_arm64> sbitmap_word 0xffff20201510a000 | grep word
struct sbitmap_word {
  word = 9223372036854775808,
crash_8.0_arm64>

// 第二个word sbitmap_word大小为192 -- 0xc0
crash_8.0_arm64> sbitmap_word 0xffff20201510a0c0 |grep word
struct sbitmap_word {
  word = 9225623836668461055,
crash_8.0_arm64>

// 0x8000000000000000
crash_8.0_arm64> sbitmap_word 0xffff20201510a180 | grep word
struct sbitmap_word {
  word = 9223372036854775808,
crash_8.0_arm64>

// 0xc000000000000000
crash_8.0_arm64> sbitmap_word 0xffff20201510a240 | grep word
struct sbitmap_word {
  word = 13835058055282163712,
crash_8.0_arm64>

crash_8.0_arm64> sbitmap_word 0xffff20201510a000 | grep cleared
  cleared = 9223372036854775808,
crash_8.0_arm64> sbitmap_word 0xffff20201510a0c0 |grep cleared
  cleared = 9223442405598937087,
crash_8.0_arm64> sbitmap_word 0xffff20201510a180 | grep cleared
  cleared = 9223372036854775808,
crash_8.0_arm64> sbitmap_word 0xffff20201510a240 | grep cleared
  cleared = 13835058055282163712,
crash_8.0_arm64>


// 第二个word
word - cleard
9225623836668461055 - 9223442405598937087 = ‭2181431069523968‬
0x‭7C00000004000‬
‭0100000000000000‬

第二个word有6个request，word内部索引为50 49 48 47 46 14
对应request索引为
50 49 48 47 46 14
+
64
=
114 113 112 111 110 78
```



查看已分配的request

**static_rqs + index --> request**

```
// 查找预分配的request
crash_8.0_arm64> blk_mq_tags 0xffff202001718200 | grep static_rqs
  static_rqs = 0xffff202015278000,
crash_8.0_arm64>

(1)
0xffff202015278000 + 78 * 8 = 0xffff202015278270

crash_8.0_arm64> rd 0xffff202015278270
ffff202015278270:  ffff2020152f1180                    ../.  ..
crash_8.0_arm64>

crash_8.0_arm64> request ffff2020152f1180
struct request {
  q = 0xffff002099bf0938,
......
  state = MQ_RQ_IDLE,
  ref = {
    refs = {
      counter = 1
    }
  },
......

crash_8.0_arm64> request ffff2020152f1180 | grep start_time_ns
  start_time_ns = 9305487068714,
  io_start_time_ns = 0,
crash_8.0_arm64>

(2)
0xffff202015278000 + 110 * 8 = 0xffff202015278370

crash_8.0_arm64> rd 0xffff202015278370
ffff202015278370:  ffff2020152f8180                    ../.  ..
crash_8.0_arm64>

crash_8.0_arm64> request ffff2020152f8180
struct request {
  q = 0xffff002099bf0938,
......
  state = MQ_RQ_IN_FLIGHT,
  ref = {
    refs = {
      counter = 1
    }
  },
......

crash_8.0_arm64> request ffff2020152f8180 | grep start_time_ns
  start_time_ns = 9306288149554,
  io_start_time_ns = 9306288223564,
crash_8.0_arm64>

(3)
0xffff202015278000 + 111 * 8 = 0xffff202015278378

crash_8.0_arm64> rd 0xffff202015278378
ffff202015278378:  ffff2020152f8500                    ../.  ..
crash_8.0_arm64>

crash_8.0_arm64> request ffff2020152f8500
struct request {
  q = 0xffff002099bf0938,
......
  state = MQ_RQ_IN_FLIGHT,
  ref = {
    refs = {
      counter = 1
    }
  },
......

crash_8.0_arm64> request ffff2020152f8500 | grep start_time_ns
  start_time_ns = 9306288273894,
  io_start_time_ns = 9306288334134,
crash_8.0_arm64>

(4)
0xffff202015278000 + 112 * 8 = 0xffff202015278380

crash_8.0_arm64> rd 0xffff202015278380
ffff202015278380:  ffff2020152f8880                    ../.  ..
crash_8.0_arm64>

crash_8.0_arm64> request ffff2020152f8880
struct request {
  q = 0xffff002099bf0938,
......
  state = MQ_RQ_IN_FLIGHT,
  ref = {
    refs = {
      counter = 1
    }
  },
......

crash_8.0_arm64> request ffff2020152f8880 | grep start_time_ns
  start_time_ns = 9306288384874,
  io_start_time_ns = 9306288567754,
crash_8.0_arm64>


(5)
0xffff202015278000 + 113 * 8 = 0xffff202015278388

crash_8.0_arm64> rd 0xffff202015278388
ffff202015278388:  ffff2020152f8c00                    ../.  ..
crash_8.0_arm64>

crash_8.0_arm64> request ffff2020152f8c00
struct request {
  q = 0xffff002099bf0938,
......
  state = MQ_RQ_IN_FLIGHT,
  ref = {
    refs = {
      counter = 1
    }
  },
......

crash_8.0_arm64> request ffff2020152f8c00 | grep start_time_ns
  start_time_ns = 9306288618684,
  io_start_time_ns = 9306288684074,
crash_8.0_arm64>


(6)
0xffff202015278000 + 114 * 8 = 0xffff202015278390

crash_8.0_arm64> rd 0xffff202015278390
ffff202015278390:  ffff2020152f8f80                    ../.  ..
crash_8.0_arm64>

crash_8.0_arm64> request ffff2020152f8f80
struct request {
  q = 0xffff002099bf0938,
......
  state = MQ_RQ_IDLE,
  ref = {
    refs = {
      counter = 1
    }
  },
......

crash_8.0_arm64> request ffff2020152f8f80 | grep start_time_ns
  start_time_ns = 9306288735394,
  io_start_time_ns = 0,
crash_8.0_arm64>

[ 9307.792759] INFO: task dio_truncate:1784076 blocked for more than 120 seconds.
[ 9307.801099] Tainted: G W OE 5.10.0+ #1
[ 9307.807463] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[ 9307.816337] task:dio_truncate state:D stack: 0 pid:1784076 ppid:1784075 flags:0x00000200
[ 9307.826010] Call trace:
[ 9307.829545] __switch_to+0x7c/0xbc
[ 9307.834035] __schedule+0x424/0x850

与dmesg对比，request初始化时间与hungtask触发时间只差2秒左右，所以hungtask并非request未及时完成导致
```

#### （3）确认进程IO状态

当前持锁进程未及时完成的两种可能：

1）进程IO未及时下发(一部分已下发生成request，一部分未下发)

2）进程IO正常下发，但IO时延过大

3）进程未及时执行(当前进程拿到文件锁时，后续阻塞进程已长时间阻塞，当前进程在正常执行时，触发了hungtask)



**gendisk --> hd_struct --> disk_stats --> in_flight**

**只有bio based的盘可以通过这种方式查看IO状态**

**request based的盘通过request的stat查看状态**

**当前为dm(bio based)叠加scsi(request based)的场景**

```
crash_8.0_arm64> dev -d
MAJOR GENDISK            NAME       REQUEST_QUEUE      TOTAL ASYNC  SYNC
    8 ffff20200745f800   sdc        ffff002094b28000       0     0     0
    8 ffff202009408000   sdb        ffff002094b29270       0     0     0
    8 ffff202006986800   sdd        ffff002099bf0938       6     0     6
    8 ffff20200777c800   sda        ffff002094b28938       0     0     0
    8 ffff202014feb000   sdf        ffff002099bf3750       0     0     0
    8 ffff2020092e6000   sdg        ffff00208e619270       0     0     0
    8 ffff20200949a000   sde        ffff002099bf0000       0     0     0
    8 ffff2020092c7000   sdi        ffff202014ff8000       0     0     0
    8 ffff2020092cb000   sdh        ffff00208e618938       0     0     0
    8 ffff202014f27800   sdk        ffff20201505e568       0     0     0
    8 ffff202006a7e800   sdl        ffff2020150ce568       0     0     0
    8 ffff202000a54800   sdj        ffff20201505dc30       0     0     0
  253 ffff004007739000   dm-0       ffff00401253d2f8       1     0     1
  253 ffff00400a422000   dm-1       ffff00400a5452f8       0     0     0
  253 ffff204001fab800   dm-2       ffff202017f90938       5     5     0
crash_8.0_arm64>

crash_8.0_arm64> gendisk ffff204001fab800
struct gendisk {
  major = 253,
  first_minor = 2,
  minors = 1,
  disk_name = "dm-2\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000",
  events = 0,
  event_flags = 0,
  part_tbl = 0xffff20400179c480,
  part0 = {
    start_sect = 0,
    nr_sects = 3592224768,
    stamp = 4297218869,
    dkstats = 0x5da7d1006330, // 磁盘状态
    ref = {
      percpu_count_ptr = 102975347365488,
      data = 0xffff20400179c380
    },

// 查找percpu基址
crash_8.0_arm64> p __per_cpu_offset
__per_cpu_offset = $1 =
 {18446603576443760640, 18446603576443904000, 18446603576444047360, 18446603576444190720, 18446603576444334080, 18446603576444477440, 18446603576444620800, 18446603576444764160, 18446603576444907520, 18446603576445050880, 18446603576445194240, 18446603576445337600, 18446603576445480960, 18446603576445624320, 18446603576445767680, 18446603576445911040, 18446603576446054400, 18446603576446197760, 18446603576446341120, 18446603576446484480, 18446603576446627840, 18446603576446771200, 18446603576446914560, 18446603576447057920, 18446603713882722304, 18446603713882865664, 18446603713883009024, 18446603713883152384, 18446603713883295744, 18446603713883439104, 18446603713883582464, 18446603713883725824, 18446603713883869184, 18446603713884012544, 18446603713884155904, 18446603713884299264, 18446603713884442624, 18446603713884585984, 18446603713884729344, 18446603713884872704, 18446603713885016064, 18446603713885159424, 18446603713885302784, 18446603713885446144, 18446603713885589504, 18446603713885732864, 18446603713885876224, 18446603713886019584, 18446638760815849472, 18446638760815992832, 18446638760816136192, 18446638760816279552, 18446638760816422912, 18446638760816566272, 18446638760816709632, 18446638760816852992, 18446638760816996352, 18446638760817139712, 18446638760817283072, 18446638760817426432, 18446638760817569792, 18446638760817713152, 18446638760817856512, 18446638760817999872, 18446638760818143232, 18446638760818286592, 18446638760818429952, 18446638760818573312, 18446638760818716672, 18446638760818860032, 18446638760819003392, 18446638760819146752, 18446638898254864384, 18446638898255007744, 18446638898255151104, 18446638898255294464, 18446638898255437824, 18446638898255581184, 18446638898255724544, 18446638898255867904, 18446638898256011264, 18446638898256154624, 18446638898256297984, 18446638898256441344, 18446638898256584704, 18446638898256728064, 18446638898256871424, 18446638898257014784, 18446638898257158144, 18446638898257301504, 18446638898257444864, 18446638898257588224, 18446638898257731584, 18446638898257874944, 18446638898258018304, 18446638898258161664, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...}

// 查看所有CPU上的处于in_flight状态的IO
disk_stats 0xffffdddfbf61d330 | grep counter >> percpu.txt
...
disk_stats 0xfffffdffbf951330 | grep counter >> percpu.txt
 
cat percpu | awk 'BEGIN{sum=0} {sum+=$3} END{printf("%d\n", sum);}'
5

累计5个
```

**IO数量与request数量一致，进程IO正常下发**

```
// 获取IO总数与总时延
disk_stats 0xffffdddfbf61d330 | grep nsecs >> nsecs.txt
...
disk_stats 0xfffffdffbf951330 | grep nsecs >> nsecs.txt

disk_stats 0xffffdddfbf61d330 | grep ios >> ios.txt
...
disk_stats 0xfffffdffbf951330 | grep ios >> ios.txt

// 提取数字
cat nsecs.txt | awk -F '[ ,{}]'  '{print $6,$8,$10,$12}' > nsecs.log
cat ios.txt | awk -F '[ ,{}]'  '{print $6,$8,$10,$12}' > ios.log

// 累计
cat ios.log | awk 'BEGIN{sum=0} {sum+=$1;sum+=$2;sum+=$3;sum+=$4} END{printf("%d\n", sum);}'
7887638
cat nsecs.log | awk 'BEGIN{sum=0} {sum+=$1;sum+=$2;sum+=$3;sum+=$4} END{printf("%d\n", sum);}'
14471216000000

// 平均时延
14471216000000 / 7887638 = ‭1,834,670.404498786582244266280983‬

约1.8ms，时延正常
```

**IO时延正常**



当前进展：
5个读进程IO未完成，未释放信号量，导致后续1个写进程及11个读进程hungtask；
5个读进程的IO未卡主，下发到驱动的5个对应request在hungtask前2秒刚生成。
从vmcore中未发现异常，已尝试复现超1个月，未复现问题
问题场景：
理论分析，在多进程读写同一个文件的场景下，存在部分进程长时间拿不到信号量而hung住的可能
问题影响：
部分进程hung住，需等待较长时间才能完成IO

### 查看throttle阻塞IO

```
// CONFIG_BLK_DEV_THROTTLING
crash> request_queue.td ffff888116e852f0
  td = 0xffff88811c92c000,
crash> throtl_data 0xffff88811c92c000 | grep nr_queued
    nr_queued = {0, 0},
  nr_queued = {0, 0},
crash>

// rq_qos
crash> request_queue ffff888116e852f0 | grep rq_qos
  rq_qos = 0xffff888131f08e58,
crash> rq_qos 0xffff888131f08e58
struct rq_qos {
  ops = 0xffffffff84aacca0 <wbt_rqos_ops>,
  q = 0xffff888116e852f0,
  id = RQ_QOS_WBT,
  next = 0x0,
  debugfs_dir = 0xffff888109415288
}
crash> struct -o rq_wb
struct rq_wb {
    [0] unsigned int wb_background;
    [4] unsigned int wb_normal;
    [8] short enable_state;
   [12] unsigned int unknown_cnt;
   [16] u64 win_nsec;
   [24] u64 cur_win_nsec;
   [32] struct blk_stat_callback *cb;
   [40] u64 sync_issue;
   [48] void *sync_cookie;
   [56] unsigned int wc;
   [64] unsigned long last_issue;
   [72] unsigned long last_comp;
   [80] unsigned long min_lat_nsec;
   [88] struct rq_qos rqos;
  [128] struct rq_wait rq_wait[3];
  [272] struct rq_depth rq_depth;
}
SIZE: 296
crash>
crash> rq_wb.rq_wait 0xffff888131f08e00
  rq_wait = {{
      wait = {
        lock = {
          {
            rlock = {
              raw_lock = {
                {
                  val = {
                    counter = 0
                  },
                  {
                    locked = 0 '\000',
                    pending = 0 '\000'
                  },
                  {
                    locked_pending = 0,
                    tail = 0
                  }
                }
              },
              magic = 3735899821,
              owner_cpu = 4294967295,
              owner = 0xffffffffffffffff
            }
          }
        },
        head = {
          next = 0xffff888131f08e98,
          prev = 0xffff888131f08e98
        }
...
crash> list 0xffff888131f08e98
ffff888131f08e98
crash>
```



### 查看percpu变量

https://www.cnblogs.com/pengdonglin137/p/17724719.html

```
crash> kmem -o
PER-CPU OFFSET VALUES:
  CPU 0: ffff8881f7200000
  CPU 1: ffff8881f7280000
  CPU 2: ffff8881f7300000
  CPU 3: ffff8881f7380000
crash>
crash> request_queue.q_usage_counter ffff888116e852f0
  q_usage_counter = {
    percpu_count_ptr = 106094426872408,
    data = 0xffff888110871c40
  },
crash> eval 106094426872408
hexadecimal: 607e08a05a58
    decimal: 106094426872408
      octal: 3007701050055130
     binary: 0000000000000000011000000111111000001000101000000101101001011000
crash> ptov 607e08a05a58:0
PER-CPU OFFSET: 607e08a05a58
  CPU    VIRTUAL
  [0]  ffffe8ffffc05a58
crash> ptov 607e08a05a58:1
PER-CPU OFFSET: 607e08a05a58
  CPU    VIRTUAL
  [1]  ffffe8ffffc85a58
crash> ptov 607e08a05a58:2
PER-CPU OFFSET: 607e08a05a58
  CPU    VIRTUAL
  [2]  ffffe8ffffd05a58
crash> ptov 607e08a05a58:3
PER-CPU OFFSET: 607e08a05a58
  CPU    VIRTUAL
  [3]  ffffe8ffffd85a58
crash> rd ffffe8ffffc05a58
ffffe8ffffc05a58:  0000000000000000                    ........
crash> rd ffffe8ffffc85a58
ffffe8ffffc85a58:  0000000000000000                    ........
crash> rd ffffe8ffffd05a58
ffffe8ffffd05a58:  0000000000000000                    ........
crash> rd ffffe8ffffd85a58
ffffe8ffffd85a58:  0000000000000000                    ........
crash>

```



```
crash> p callback_wq
callback_wq = $1 = (struct workqueue_struct *) 0xffff0000c30a1400
crash>
crash> workqueue_struct.cpu_pwq 0xffff0000c30a1400
  cpu_pwq = 0xccfe9cb5d8d0
crash> kmem -o
PER-CPU OFFSET VALUES:
  CPU 0: ffff2f015341c000
  CPU 1: ffff2f0153442000
  CPU 2: ffff2f0153468000
  CPU 3: ffff2f015348e000
crash>
// ffff2f015341c000 + ccfe9cb5d8d0  = FFFFFBFFEFF798D0
crash> rd FFFFFBFFEFF798D0
fffffbffeff798d0:  ffff0000d3488d00                    ..H.....
crash>
// ffff2f0153442000 + ccfe9cb5d8d0 = FFFFFBFFEFF9F8D0
crash> rd FFFFFBFFEFF9F8D0
fffffbffeff9f8d0:  ffff0000d3488d00                    ..H.....
crash>
// ffff2f0153468000 + ccfe9cb5d8d0 = FFFFFBFFEFFC58D0
crash> rd FFFFFBFFEFFC58D0
fffffbffeffc58d0:  ffff0000d3488d00                    ..H.....
crash>
// ffff2f015348e000 + ccfe9cb5d8d0 = FFFFFBFFEFFEB8D0
crash> rd FFFFFBFFEFFEB8D0
fffffbffeffeb8d0:  ffff0000d3488d00                    ..H.....
crash>

crash> pool_workqueue.pool ffff0000d3488d00
  pool = 0xffff0000c01b6800,
crash>
crash> worker_pool.worklist 0xffff0000c01b6800
  worklist = {
    next = 0xffff0000c906c4a8,
    prev = 0xffffd0ff8944fc68 <stats_flush_dwork+8>
  },
crash>

crash> list 0xffff0000c906c4a8
ffff0000c906c4a8
ffffd0ff8944fc68
ffff0000c01b6860
crash>

crash> work_struct.func ffff0000c906c4a0
  func = 0xffffd0ff84fae128 <wb_update_bandwidth_workfn>,
crash> work_struct.func 0xffffd0ff8944fc60
  func = 0xffffd0ff8510b258 <flush_memcg_stats_dwork>,
crash>

workqueue上就两个work，和nfsd无关
```





```
同一个gendisk的所有硬件队列使用的是同一个tags，同一个sched_tags(与调度器对应)
不同的gendisk的硬件队列也可能使用同一个sched_tags


// crash读取percpu变量
crash_8.0_arm64> request_queue 0xffff002099bf0938
struct request_queue {
  last_merge = 0xffff2020152f8f80,
  elevator = 0xffff20200149b800,
  q_usage_counter = {
    percpu_count_ptr = 102975354249432,
    data = 0xffff00208f3b2f80
  },
  stats = 0xffff00208f3b2e00,

// percpu变量偏移为102975354249432

// 查找percpu基址
crash_8.0_arm64> p __per_cpu_offset
__per_cpu_offset = $1 =
 {18446603576443760640, 18446603576443904000, 18446603576444047360, 18446603576444190720, 18446603576444334080, 18446603576444477440, 18446603576444620800, 18446603576444764160, 18446603576444907520, 18446603576445050880, 18446603576445194240, 18446603576445337600, 18446603576445480960, 18446603576445624320, 18446603576445767680, 18446603576445911040, 18446603576446054400, 18446603576446197760, 18446603576446341120, 18446603576446484480, 18446603576446627840, 18446603576446771200, 18446603576446914560, 18446603576447057920, 18446603713882722304, 18446603713882865664, 18446603713883009024, 18446603713883152384, 18446603713883295744, 18446603713883439104, 18446603713883582464, 18446603713883725824, 18446603713883869184, 18446603713884012544, 18446603713884155904, 18446603713884299264, 18446603713884442624, 18446603713884585984, 18446603713884729344, 18446603713884872704, 18446603713885016064, 18446603713885159424, 18446603713885302784, 18446603713885446144, 18446603713885589504, 18446603713885732864, 18446603713885876224, 18446603713886019584, 18446638760815849472, 18446638760815992832, 18446638760816136192, 18446638760816279552, 18446638760816422912, 18446638760816566272, 18446638760816709632, 18446638760816852992, 18446638760816996352, 18446638760817139712, 18446638760817283072, 18446638760817426432, 18446638760817569792, 18446638760817713152, 18446638760817856512, 18446638760817999872, 18446638760818143232, 18446638760818286592, 18446638760818429952, 18446638760818573312, 18446638760818716672, 18446638760818860032, 18446638760819003392, 18446638760819146752, 18446638898254864384, 18446638898255007744, 18446638898255151104, 18446638898255294464, 18446638898255437824, 18446638898255581184, 18446638898255724544, 18446638898255867904, 18446638898256011264, 18446638898256154624, 18446638898256297984, 18446638898256441344, 18446638898256584704, 18446638898256728064, 18446638898256871424, 18446638898257014784, 18446638898257158144, 18446638898257301504, 18446638898257444864, 18446638898257588224, 18446638898257731584, 18446638898257874944, 18446638898258018304, 18446638898258161664, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...}


// 读取percpu变量
crash_8.0_arm64> rd 0xffffdddfbfca90d8
ffffdddfbfca90d8:  fffffffffffdb146                    F.......

-151226


```



### 命令执行

./crash ~/crash/core/oom_iouring/vmlinux ~/crash/core/oom_iouring/vmcore -i res.txt

### 分配内存查询

```
// 查分配的bio
crash> p bio_slabs
bio_slabs = $5 = (struct bio_slab *) 0xffff888107bcb6c8
crash> p bio_slab_nr
bio_slab_nr = $6 = 3
crash> bio_slab 0xffff888107bcb6c8
struct bio_slab {
  slab = 0xffff888102f6a100,
  slab_ref = 25,
  slab_size = 216,
  name = "bio-0\000\000"
}
crash> bio_slab 0xffff888107bcb6e0
struct bio_slab {
  slab = 0xffff888102f6a900,
  slab_ref = 1,
  slab_size = 288,
  name = "bio-1\000\000"
}
crash> bio_slab 0xffff888107bcb6f8
struct bio_slab {
  slab = 0xffff888107718900,
  slab_ref = 1,
  slab_size = 240,
  name = "bio-2\000kk"
}
crash>

crash> p fs_bio_set
fs_bio_set = $7 = {
  bio_slab = 0xffff888102f6a100,
  front_pad = 0,
...
对应 bio-0

crash> kmem -S bio-0
```



```
cat /proc/slabinfo
kmalloc-8k            52     52  16384    2    8 : tunables    0    0    0 : slabdata     26     26      0
kmalloc-4k         19088  19088   8192    4    8 : tunables    0    0    0 : slabdata   4772   4772      0
kmalloc-2k          2936   2936   4096    8    8 : tunables    0    0    0 : slabdata    367    367      0
kmalloc-1k          5360   5360   2048   16    8 : tunables    0    0    0 : slabdata    335    335      0
kmalloc-512        36848  36848   1024   16    4 : tunables    0    0    0 : slabdata   2303   2303      0
kmalloc-256         1264   1264    512   16    2 : tunables    0    0    0 : slabdata     79     79      0
kmalloc-192         8400   8400    256   16    1 : tunables    0    0    0 : slabdata    525    525      0
kmalloc-128         4224   4224    256   16    1 : tunables    0    0    0 : slabdata    264    264      0
kmalloc-96         19328  19328    128   32    1 : tunables    0    0    0 : slabdata    604    604      0
kmalloc-64         41888  41888    128   32    1 : tunables    0    0    0 : slabdata   1309   1309      0
kmalloc-32         15488  15488     64   64    1 : tunables    0    0    0 : slabdata    242    242      0
kmalloc-16          5632   5632     32  128    1 : tunables    0    0    0 : slabdata     44     44      0
kmalloc-8          13974  13974     40  102    1 : tunables    0    0    0 : slabdata    137    137      0

```





## gdb调试

gdb vmlinux

查看汇编
disassemble io_prep_async_work


# 9、其他

pahole -C kernfs_open_file vmlinux
通过vmlinux查看结构体成员偏移

pahole -E -C ata_link vmlinux | less

git grep kmem_cache_create fs/kernfs/
查看关键词位置

cat /proc/kallsyms | grep origin
全局符号地址

# 10、IO性能问题

查看磁盘调度器
cat /sys/block/sda/queue/scheduler

查看磁盘配置
find /sys/block/sda/ -type f | xargs grep . 

```
1、确认IO是否是关键路径
1) iostat -x 10 // 以10秒为周期查看带宽，可以适当延长周期
2) 通过cgroup限速
// cgroup v1
mkdir -p /sys/fs/cgroup/blkio/throt // 创建新的blk-cgroup
echo "8:0 1000" > /sys/fs/cgroup/blkio/throt/blkio.throttle.read_bps_device // 设置磁盘和IO限制，IO限制设置为iostat查询到的稳定带宽的90%
echo $$ > /sys/fs/cgroup/blkio/throt/cgroup.procs // 将下发IO的进程加入控制，"$$"改为对应进程号
// cgroup v2
mkdir -p /sys/fs/cgroup/blktest
echo "8:0 rbps=2" > /sys/fs/cgroup/blktest/io.max
echo $$ > cgroup.procs
3) IO下发完成后查看TPS是否有明显变化

2、查看盘配置（IO确认是关键路径）
find /sys/block/sda/ -type f | xargs grep .
查看磁盘配置并对比

3、blktrace抓取IO耗时情况（IO确认是关键路径，且配置无差异）
通过iostat观察带宽是否稳定，在带宽稳定的情况下抓取IO耗时情况（通过延长测试时间或其他业务层手段保证带宽稳定）

sh bmetric.sh -s Q 418.log > 418_Q_res.log
sh bmetric.sh -s D 418.log > 418_D_res.log
sh bmetric.sh -s Q 510_none.log > 510_none_Q_res.log
sh bmetric.sh -s D 510_none.log > 510_none_D_res.log

过滤Q-C和D-C的数据对比
```


# 11、iostat

iostat -dmx 1 sda

https://www.cnblogs.com/ggjucheng/archive/2013/01/13/2858810.html


```
[root@localhost ~]# iostat sda
Linux 5.10.0-00001-gc7bfee48e7d2-dirty (localhost.localdomain)  07/01/2024      _x86_64_        (16 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           0.05    0.00    7.06    0.06    0.02   92.80

Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn
sda               0.18         3.75         0.00       2104          0

tps：该设备每秒的传输次数（Indicate the number of transfers per second that were issued to the device.）。"一次传输"意思是"一次I/O请求"。多个逻辑请求可能会被合并为"一次I/O请求"。"一次传输"请求的大小是未知的。
kB_read/s：每秒从设备（drive expressed）读取的数据量；
kB_wrtn/s：每秒向设备（drive expressed）写入的数据量；
kB_read：读取的总数据量；
kB_wrtn：写入的总数量数据量；这些单位都为Kilobytes。

[root@localhost ~]# iostat -x sda
Linux 5.10.0-00001-gc7bfee48e7d2-dirty (localhost.localdomain)  07/01/2024      _x86_64_        (16 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           0.05    0.00    6.99    0.06    0.02   92.88

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     0.00    0.17    0.00     3.71     0.00    42.51     0.00    5.92    5.92    0.00   5.55   0.10

[root@localhost ~]#

rrqm/s：每秒这个设备相关的读取请求有多少被Merge了（当系统调用需要读取数据的时候，VFS将请求发到各个FS，如果FS发现不同的读取请求读取的是相同Block的数据，FS会将这个请求合并Merge）；wrqm/s：每秒这个设备相关的写入请求有多少被Merge了。
rsec/s：每秒读取的扇区数；
wsec/：每秒写入的扇区数。
rKB/s：The number of read requests that were issued to the device per second；
wKB/s：The number of write requests that were issued to the device per second；
avgrq-sz 平均请求扇区的大小
avgqu-sz 是平均请求队列的长度。毫无疑问，队列长度越短越好。request个数。The average queue length of the requests that were issued to the device    
await：  每一个IO请求的处理的平均时间（单位是微秒毫秒）。这里可以理解为IO的响应时间，一般地系统IO响应时间应该低于5ms，如果大于10ms就比较大了。
         这个时间包括了队列时间和服务时间，也就是说，一般情况下，await大于svctm，它们的差值越小，则说明队列时间越短，反之差值越大，队列时间越长，说明系统出了问题。 requests issued to the device to be served 通过queue_rq提交到设备上的处理时间
svctm    表示平均每次设备I/O操作的服务时间（以毫秒为单位）。如果svctm的值与await很接近，表示几乎没有I/O等待，磁盘性能很好，如果await的值远高于svctm的值，则表示I/O队列等待太长，         系统上运行的应用程序将变慢。
%util： 在统计时间内所有处理IO时间，除以总共统计时间。例如，如果统计间隔1秒，该设备有0.8秒在处理IO，而0.2秒闲置，那么该设备的%util = 0.8/1 = 80%，所以该参数暗示了设备的繁忙程度
。一般地，如果该参数是100%表示设备已经接近满负荷运行了（当然如果是多磁盘，即使%util是100%，因为磁盘的并发能力，所以磁盘使用未必就到了瓶颈）。

```

